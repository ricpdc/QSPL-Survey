{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ricpdc/QSPL-Survey/blob/main/QSE_Survey_Analysis_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf1aad4c",
      "metadata": {
        "id": "cf1aad4c"
      },
      "source": [
        "\n",
        "# QSE Hybrid Systems Survey — Reproducible Analysis (Colab)\n",
        "\n",
        "**Project:** Validation of a Feature Model for Hybrid Quantum–Classical Systems (QSE)  \n",
        "**Goal:** Load Google Sheets survey responses, clean data, produce tables/figures, and compute basic statistics (incl. reliability for Likert constructs), enabling a **re-runnable** pipeline as more responses arrive.\n",
        "\n",
        "**Last updated:** 2025-10-02 06:32 UTC  \n",
        "**Author(s):** _Add names here_\n",
        "\n",
        "---\n",
        "\n",
        "## How to use this notebook\n",
        "\n",
        "1. Ensure the survey Google Sheet is shared appropriately (see **Config** cell).  \n",
        "2. Run the notebook top-to-bottom.  \n",
        "3. All figures and tables are written to `/content/outputs` in Colab (or the `outputs` folder if you mount Drive).  \n",
        "4. The notebook is structured to be **self-contained and re-runnable**; you can refresh results as new responses arrive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "243e72a8",
      "metadata": {
        "id": "243e72a8"
      },
      "source": [
        "##**1.** Runtime & libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cfbba80",
      "metadata": {
        "id": "8cfbba80",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Install and import libraries\n",
        "# If you need pinned versions, uncomment the pip cell and pin accordingly.\n",
        "# %%capture\n",
        "# !pip install pandas==2.2.2 numpy==2.0.2 matplotlib==3.9.0 scipy==1.13.1 statsmodels==0.14.2 gspread==6.1.4 gspread-dataframe==3.3.1\n",
        "\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "import json\n",
        "import math\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# (Optional) stats\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# (Optional) Google Sheets API\n",
        "try:\n",
        "    import gspread\n",
        "    from gspread_dataframe import get_as_dataframe\n",
        "    HAS_GSPREAD = True\n",
        "except Exception as e:\n",
        "    HAS_GSPREAD = False\n",
        "\n",
        "print('pandas:', pd.__version__)\n",
        "print('numpy:', np.__version__)\n",
        "print('matplotlib:', plt.matplotlib.__version__)\n",
        "print('scipy:', stats.__version__ if hasattr(stats, '__version__') else 'n/a')\n",
        "print('statsmodels:', sm.__version__)\n",
        "print('gspread available:', HAS_GSPREAD)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e57a459e",
      "metadata": {
        "id": "e57a459e"
      },
      "source": [
        "##**2.** Configuration (Google Sheets & paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03b27d2a",
      "metadata": {
        "id": "03b27d2a"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Configuration\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# === Google Sheets ===\n",
        "# Use the SHEET_ID from the spreadsheet URL:\n",
        "# Example: https://docs.google.com/spreadsheets/d/<<SHEET_ID>>/edit?usp=sharing\n",
        "SHEET_ID = \"1t-3FYihK9DdpKpdtIgmWvYpduzWqeUkcL3p5npcCwDM\"  # Provided by the project owner\n",
        "# Optionally specify the GID for each sheet/tab if needed. The default (0) is often the first tab.\n",
        "# You can find GID by opening the specific tab and reading the 'gid=<NUMBER>' param in the URL.\n",
        "PRIMARY_GID = 0  # Change if your responses are in another tab\n",
        "\n",
        "# === Access method ===\n",
        "ACCESS_METHOD = \"csv_export\"  #@param [\"csv_export\", \"gspread_oauth\"]\n",
        "# - \"csv_export\": uses public CSV export (Sheet must be at least 'Anyone with the link: Viewer')\n",
        "# - \"gspread_oauth\": uses interactive OAuth in Colab (no need to make sheet public)\n",
        "\n",
        "# === Paths ===\n",
        "BASE_DIR = Path(\"/content\")\n",
        "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
        "CACHE_DIR = BASE_DIR / \"cache\"\n",
        "for d in (OUTPUT_DIR, CACHE_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# === Optional: Mount Google Drive ===\n",
        "MOUNT_DRIVE = False  #@param {type:\"boolean\"}\n",
        "if MOUNT_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Redirect outputs if desired:\n",
        "    # OUTPUT_DIR = Path('/content/drive/MyDrive/qse_survey/outputs')\n",
        "    # OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('OUTPUT_DIR:', OUTPUT_DIR)\n",
        "print('CACHE_DIR:', CACHE_DIR)\n",
        "print('ACCESS_METHOD:', ACCESS_METHOD)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bce2d68",
      "metadata": {
        "id": "5bce2d68"
      },
      "source": [
        "##**3.** Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69d9dd5e",
      "metadata": {
        "id": "69d9dd5e",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Helper functions to load Google Sheets (robust)\n",
        "import urllib.error\n",
        "import urllib.parse\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "\n",
        "def sheets_csv_url(sheet_id: str, gid: int = 0) -> str:\n",
        "    return f\"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&gid={gid}\"\n",
        "\n",
        "def sheets_gviz_url_by_sheetname(sheet_id: str, sheet_name: str) -> str:\n",
        "    # GVIZ endpoint supports selecting a tab by name\n",
        "    # t q x = out:csv  (forces CSV)\n",
        "    from urllib.parse import quote\n",
        "    return f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={quote(sheet_name)}\"\n",
        "\n",
        "def try_read_csv(url: str) -> pd.DataFrame:\n",
        "    # Give a clearer error if 400/403/404\n",
        "    try:\n",
        "        return pd.read_csv(url)\n",
        "    except urllib.error.HTTPError as e:\n",
        "        print(f\"[ERROR] HTTPError {e.code} for URL: {url}\")\n",
        "        if e.code in (400, 403):\n",
        "            print(\"This often means the sheet is not shared publicly OR the gid/sheet name is wrong.\")\n",
        "        raise\n",
        "    except Exception as ex:\n",
        "        print(f\"[ERROR] Could not read CSV: {ex}\")\n",
        "        raise\n",
        "\n",
        "def load_from_csv_export(sheet_id: str, gid: int = 0) -> pd.DataFrame:\n",
        "    url = sheets_csv_url(sheet_id, gid)\n",
        "    print(\"Attempting CSV export via gid:\", gid)\n",
        "    print(\"URL:\", url)\n",
        "    df = try_read_csv(url)\n",
        "    return df\n",
        "\n",
        "def load_from_gviz_by_sheetname(sheet_id: str, sheet_name: str) -> pd.DataFrame:\n",
        "    url = sheets_gviz_url_by_sheetname(sheet_id, sheet_name)\n",
        "    print(\"Attempting GVIZ CSV via sheet name:\", sheet_name)\n",
        "    print(\"URL:\", url)\n",
        "    df = try_read_csv(url)\n",
        "    return df\n",
        "\n",
        "def load_from_gspread(sheet_id: str, worksheet_index: int = 0) -> pd.DataFrame:\n",
        "    assert HAS_GSPREAD, \"gspread is not available. Install it and set ACCESS_METHOD='gspread_oauth'.\"\n",
        "    print(\"Using gspread OAuth. You may be prompted to authorize.\")\n",
        "    gc = gspread.oauth()  # Will prompt in Colab the first time\n",
        "    sh = gc.open_by_key(sheet_id)\n",
        "    ws = sh.get_worksheet(worksheet_index)\n",
        "    df = get_as_dataframe(ws, evaluate_formulas=True, header=0)\n",
        "    # Clean all-null columns that may appear due to trailing columns\n",
        "    if df.shape[1] > 0 and df.columns[-1] is None:\n",
        "        df = df.iloc[:, :-1]\n",
        "    return df\n",
        "\n",
        "# Optional diagnostics: list worksheets if OAuth is available\n",
        "def list_worksheets(sheet_id: str):\n",
        "    if not HAS_GSPREAD:\n",
        "        print(\"[INFO] gspread is not installed/available; cannot list worksheets.\")\n",
        "        return None\n",
        "    try:\n",
        "        gc = gspread.oauth()\n",
        "        sh = gc.open_by_key(sheet_id)\n",
        "        tabs = [ws.title for ws in sh.worksheets()]\n",
        "        print(\"Worksheets:\", tabs)\n",
        "        return tabs\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Could not list worksheets via gspread:\", e)\n",
        "        return None\n",
        "\n",
        "# ---- Main loading logic ----\n",
        "# You can specify SHEET_NAME if you know the tab name (e.g., 'Form Responses 1').\n",
        "SHEET_NAME = \"data\"  # o \"Form Responses 1\" según tu caso\n",
        "\n",
        "df_raw = None\n",
        "if ACCESS_METHOD == \"csv_export\":\n",
        "    try:\n",
        "        df_raw = load_from_csv_export(SHEET_ID, PRIMARY_GID)\n",
        "    except Exception as e:\n",
        "        print(\"\\n[HINT] CSV export failed. Common causes:\")\n",
        "        print(\"- The spreadsheet (or that tab) is not shared as 'Anyone with the link: Viewer'.\")\n",
        "        print(\"- 'PRIMARY_GID' does not match the actual tab's gid.\")\n",
        "        print(\"- For private data, switch to ACCESS_METHOD='gspread_oauth'.\")\n",
        "        if SHEET_NAME:\n",
        "            print(\"\\nTrying GVIZ endpoint with SHEET_NAME...\")\n",
        "            df_raw = load_from_gviz_by_sheetname(SHEET_ID, SHEET_NAME)\n",
        "        else:\n",
        "            print(\"\\nYou can set SHEET_NAME='Form Responses 1' (or your tab's name) and re-run to try GVIZ.\")\n",
        "            raise\n",
        "elif ACCESS_METHOD == \"gspread_oauth\":\n",
        "    df_raw = load_from_gspread(SHEET_ID, worksheet_index=0)\n",
        "else:\n",
        "    raise ValueError(\"ACCESS_METHOD must be one of: 'csv_export', 'gspread_oauth'\")\n",
        "\n",
        "print(\"Shape:\", df_raw.shape)\n",
        "df_raw.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8f1850c",
      "metadata": {
        "id": "f8f1850c"
      },
      "source": [
        "##**4.** Data dictionary & pre-cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a563d0d1",
      "metadata": {
        "id": "a563d0d1"
      },
      "outputs": [],
      "source": [
        "# Convertir la lista de tuplas a un diccionario\n",
        "DATA_DICTIONARY = dict([\n",
        "    (\"timestamp\", \"datetime\"),\n",
        "    (\"SPL_familiar\", \"boolean\"),\n",
        "    (\"gen_funct\", \"likert3\"),\n",
        "    (\"f_funct_classical_functionality\", \"likert5\"),\n",
        "    (\"f_funct_quantum_functionality\", \"likert5\"),\n",
        "    (\"f_funct_gate_based\", \"likert5\"),\n",
        "    (\"f_funct_adiabatic\", \"likert5\"),\n",
        "    (\"alter_funct\", \"string\"),\n",
        "    (\"gen_algo\", \"likert3\"),\n",
        "    (\"f_algo_ftqc\", \"likert5\"),\n",
        "    (\"f_algo_ftqc_deutsch_jozsa\", \"likert5\"),\n",
        "    (\"f_algo_ftqc_bernstein_varizani\", \"likert5\"),\n",
        "    (\"f_algo_ftqc_grover\", \"likert5\"),\n",
        "    (\"f_algo_ftqc_simon\", \"likert5\"),\n",
        "    (\"f_algo_ftqc_shor\", \"likert5\"),\n",
        "    (\"f_algo_ftqc_qft\", \"likert5\"),\n",
        "    (\"f_algo_ftqc_qpe\", \"likert5\"),\n",
        "    (\"f_algo_ftqc_hhl\", \"likert5\"),\n",
        "    (\"f_algo_nisq\", \"likert5\"),\n",
        "    (\"f_algo_nisq_qaoa\", \"likert5\"),\n",
        "    (\"f_algo_nisq_vqe\", \"likert5\"),\n",
        "    (\"f_algo_nisq_qml\", \"likert5\"),\n",
        "    (\"f_algo_hybrid\", \"likert5\"),\n",
        "    (\"f_algo_hybrid_q_as_annealing\", \"likert5\"),\n",
        "    (\"f_algo_hybrid_clustering\", \"likert5\"),\n",
        "    (\"f_algo_hybrid_qaoa\", \"likert5\"),\n",
        "    (\"f_algo_classical_algorithm\", \"likert5\"),\n",
        "    (\"c_algo_1\", \"likert5\"),\n",
        "    (\"alter_algo\", \"string\"),\n",
        "    (\"gen_lang\", \"likert3\"),\n",
        "    (\"f_lang_quantum_framework\", \"likert5\"),\n",
        "    (\"f_lang_qiskit\", \"likert5\"),\n",
        "    (\"f_lang_ocean_sdk\", \"likert5\"),\n",
        "    (\"f_lang_cirq\", \"likert5\"),\n",
        "    (\"f_lang_braket_sdk\", \"likert5\"),\n",
        "    (\"f_lang_quantum_language\", \"likert5\"),\n",
        "    (\"f_lang_qasm\", \"likert5\"),\n",
        "    (\"f_lang_q_sharp\", \"likert5\"),\n",
        "    (\"f_lang_classical_language\", \"likert5\"),\n",
        "    (\"f_lang_python\", \"likert5\"),\n",
        "    (\"f_lang_c_sharp\", \"likert5\"),\n",
        "    (\"c_lang_1\", \"likert5\"),\n",
        "    (\"c_lang_2\", \"likert5\"),\n",
        "    (\"c_lang_3\", \"likert5\"),\n",
        "    (\"c_lang_4\", \"likert5\"),\n",
        "    (\"alter_lang\", \"string\"),\n",
        "    (\"gen_hw\", \"likert3\"),\n",
        "    (\"f_hw_error_correction\", \"likert5\"),\n",
        "    (\"f_hw_active_error_correction\", \"likert5\"),\n",
        "    (\"f_hw_surface_code\", \"likert5\"),\n",
        "    (\"f_hw_bacon_shor_code\", \"likert5\"),\n",
        "    (\"f_hw_ldpc_code\", \"likert5\"),\n",
        "    (\"f_hw_error_mitigation\", \"likert5\"),\n",
        "    (\"f_hw_zero_noise_extrapolation\", \"likert5\"),\n",
        "    (\"f_hw_readout_mitigation\", \"likert5\"),\n",
        "    (\"f_hw_probabilistic_error_cancellation\", \"likert5\"),\n",
        "    (\"f_hw_native_gate_set\", \"likert5\"),\n",
        "    (\"f_hw_single_qubit_gate\", \"likert5\"),\n",
        "    (\"f_hw_two_qubits_gate\", \"likert5\"),\n",
        "    (\"f_hw_pulse_control\", \"likert5\"),\n",
        "    (\"alter_hw\", \"string\"),\n",
        "    (\"gen_integ\", \"likert3\"),\n",
        "    (\"f_integ_qcp_aa_s\", \"likert5\"),\n",
        "    (\"f_integ_qf_aa_s\", \"likert5\"),\n",
        "    (\"f_integ_orch_workflows\", \"likert5\"),\n",
        "    (\"c_integ_1\", \"likert5\"),\n",
        "    (\"alter_integ\", \"string\"),\n",
        "    (\"ccc_funct_1\", \"likert3\"),\n",
        "    (\"ccc_funct_2\", \"likert3\"),\n",
        "    (\"ccc_algo_1\", \"likert3\"),\n",
        "    (\"ccc_algo_2\", \"likert3\"),\n",
        "    (\"ccc_algo_3\", \"likert3\"),\n",
        "    (\"ccc_algo_4\", \"likert3\"),\n",
        "    (\"ccc_lang_1\", \"likert3\"),\n",
        "    (\"ccc_lang_2\", \"likert3\"),\n",
        "    (\"ccc_lang_3\", \"likert3\"),\n",
        "    (\"ccc_lang_4\", \"likert3\"),\n",
        "    (\"ccc_lang_5\", \"likert3\"),\n",
        "    (\"ccc_hw_1\", \"likert3\"),\n",
        "    (\"ccc_hw_2\", \"likert3\"),\n",
        "    (\"ccc_hw_3\", \"likert3\"),\n",
        "    (\"ccc_integ_1\", \"likert3\"),\n",
        "    (\"ccc_integ_2\", \"likert3\"),\n",
        "    (\"ccc_integ_3\", \"likert3\"),\n",
        "    (\"alter_ccc\", \"string\"),\n",
        "    (\"alter_gen\", \"string\"),\n",
        "    (\"phase\", \"category\"),\n",
        "    (\"stars\", \"likert5\"),\n",
        "    (\"role\", \"category\"),\n",
        "    (\"exp_q\", \"category\"),\n",
        "    (\"exp_spl\", \"category\"),\n",
        "    (\"qc_areas\", \"category\"),\n",
        "    (\"anon\", \"boolean\"),\n",
        "    (\"first_name\", \"string\"),\n",
        "    (\"last_name\", \"string\"),\n",
        "    (\"title\", \"string\"),\n",
        "    (\"affiliation\", \"string\"),\n",
        "    (\"country\", \"category\"),\n",
        "    (\"email\", \"string\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Mapeo para los valores textuales de Likert (convertir en enteros)\n",
        "LIKERT_5 = {\n",
        "    \"DK/NA\": 0,\n",
        "    \"1\": 1,\n",
        "    \"2\": 2,\n",
        "    \"3\": 3,\n",
        "    \"4\": 4,\n",
        "    \"5\": 5,\n",
        "}\n",
        "\n",
        "LIKERT_3 = {\n",
        "    \"DK/NA\": 0,\n",
        "    \"No\": 1,\n",
        "    \"Partially\": 2,\n",
        "    \"Yes\": 3,\n",
        "}\n",
        "\n",
        "# Función para mapear y limpiar las columnas\n",
        "def canonicalize_columns(df: pd.DataFrame, dictionary: dict) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    for raw, typ in dictionary.items():\n",
        "        if raw not in out.columns:\n",
        "            print(f\"[WARN] Missing expected column in sheet: {raw}\")\n",
        "            continue\n",
        "        new = raw  # Usamos el nombre original de la columna\n",
        "        out = out.rename(columns={raw: new})\n",
        "\n",
        "        # Procesamiento de tipos\n",
        "        if typ == \"datetime\":\n",
        "            out[new] = pd.to_datetime(out[new], errors=\"coerce\")\n",
        "        elif typ == \"numeric\":\n",
        "            out[new] = pd.to_numeric(out[new], errors=\"coerce\")\n",
        "        elif typ == \"category\":\n",
        "            out[new] = out[new].astype(\"category\")\n",
        "        elif typ == \"likert5\":\n",
        "            out[new] = out[new]\n",
        "            # Solo reemplazamos NaN por 0, sin afectar los valores ya existentes (1, 2, 3, etc.)\n",
        "            out[new] = pd.to_numeric(out[new], errors=\"coerce\").fillna(0).astype(\"Int64\")  # Usar 0 para NaNs\n",
        "        elif typ == \"likert3\":\n",
        "            # Solución para mapeo en likert3: reemplazar valores desconocidos por 0\n",
        "            out[new] = out[new].apply(lambda x: LIKERT_3.get(x, 0)).astype(\"Int64\")  # Usar 0 para valores no mapeados\n",
        "        elif typ == \"boolean\":\n",
        "            out[new] = out[new].apply(lambda x: True if str(x).strip().lower() == \"yes\" else False)\n",
        "        elif typ == \"string\":\n",
        "            out[new] = out[new].astype(\"str\")\n",
        "\n",
        "    return out\n",
        "\n",
        "# Llamamos la función y mostramos los primeros registros\n",
        "df = canonicalize_columns(df_raw, DATA_DICTIONARY)\n",
        "print(\"Columns:\", list(df.columns))\n",
        "print(\"Rows:\", len(df))\n",
        "df.head(5)\n"
      ],
      "metadata": {
        "id": "8ecBjNXXoGkm"
      },
      "id": "8ecBjNXXoGkm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e9997a2d",
      "metadata": {
        "id": "e9997a2d"
      },
      "source": [
        "##**5**. Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CONFIGURA AQUÍ LAS COLUMNAS QUE QUIERES EVALUAR\n",
        "# ============================================================\n",
        "# Ejemplos:\n",
        "# ITEMS = [c for c in df.columns if c.startswith('Functionality_')]\n",
        "# ITEMS = [\"item1\",\"item2\",\"item3\",\"item4\"]\n",
        "ITEMS = [\"gen_funct\", \"f_funct_classical_functionality\", \"f_funct_quantum_functionality\", \"f_funct_gate_based\", \"f_funct_adiabatic\", \"gen_algo\", \"f_algo_ftqc\", \"f_algo_ftqc_deutsch_jozsa\", \"f_algo_ftqc_bernstein_varizani\", \"f_algo_ftqc_grover\", \"f_algo_ftqc_simon\", \"f_algo_ftqc_shor\", \"f_algo_ftqc_qft\", \"f_algo_ftqc_qpe\", \"f_algo_ftqc_hhl\", \"f_algo_nisq\", \"f_algo_nisq_qaoa\", \"f_algo_nisq_vqe\", \"f_algo_nisq_qml\", \"f_algo_hybrid\", \"f_algo_hybrid_q_as_annealing\", \"f_algo_hybrid_clustering\", \"f_algo_hybrid_qaoa\", \"f_algo_classical_algorithm\", \"c_algo_1\", \"gen_lang\", \"f_lang_quantum_framework\", \"f_lang_qiskit\", \"f_lang_ocean_sdk\", \"f_lang_cirq\", \"f_lang_braket_sdk\", \"f_lang_quantum_language\", \"f_lang_qasm\", \"f_lang_q_sharp\", \"f_lang_classical_language\", \"f_lang_python\", \"f_lang_c_sharp\", \"c_lang_1\", \"c_lang_2\", \"c_lang_3\", \"c_lang_4\", \"gen_hw\", \"f_hw_error_correction\", \"f_hw_active_error_correction\", \"f_hw_surface_code\", \"f_hw_bacon_shor_code\", \"f_hw_ldpc_code\", \"f_hw_error_mitigation\", \"f_hw_zero_noise_extrapolation\", \"f_hw_readout_mitigation\", \"f_hw_probabilistic_error_cancellation\", \"f_hw_native_gate_set\", \"f_hw_single_qubit_gate\", \"f_hw_two_qubits_gate\", \"f_hw_pulse_control\", \"gen_integ\", \"f_integ_qcp_aa_s\", \"f_integ_qf_aa_s\", \"f_integ_orch_workflows\", \"c_integ_1\", \"ccc_funct_1\", \"ccc_funct_2\", \"ccc_algo_1\", \"ccc_algo_2\", \"ccc_algo_3\", \"ccc_algo_4\", \"ccc_lang_1\", \"ccc_lang_2\", \"ccc_lang_3\", \"ccc_lang_4\", \"ccc_lang_5\", \"ccc_hw_1\", \"ccc_hw_2\", \"ccc_hw_3\", \"ccc_integ_1\", \"ccc_integ_2\", \"ccc_integ_3\", \"stars\"]  # cámbialo por tus columnas\n"
      ],
      "metadata": {
        "id": "y6jcCtGUskGp"
      },
      "id": "y6jcCtGUskGp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "700f4954",
      "metadata": {
        "id": "700f4954"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Overview and missingness\n",
        "display(df.info())\n",
        "display(df.describe(include='all'))\n",
        "missing = df.isna().mean().sort_values(ascending=False)\n",
        "display(missing.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**6.** Analysis for individual features and intra-constraints"
      ],
      "metadata": {
        "id": "6G2gH8CEocSS"
      },
      "id": "6G2gH8CEocSS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Variables Grouping"
      ],
      "metadata": {
        "id": "lg_pq4_eY40W"
      },
      "id": "lg_pq4_eY40W"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "# Convert DATA_DICTIONARY to OrderedDict to preserve order\n",
        "dict_ordered = OrderedDict(DATA_DICTIONARY)\n",
        "\n",
        "# Flattened groups: each key -> list of Likert5 columns (features + constraints)\n",
        "groups = OrderedDict()\n",
        "current_group = None\n",
        "current_columns = []\n",
        "\n",
        "for col, typ in dict_ordered.items():\n",
        "    if typ == \"likert3\" and col.startswith(\"gen_\"):\n",
        "        if current_group:\n",
        "            groups[current_group] = current_columns\n",
        "        current_group = col\n",
        "        current_columns = []\n",
        "    elif typ == \"string\" and col.startswith(\"alter_\"):\n",
        "        if current_group:\n",
        "            groups[current_group] = current_columns\n",
        "        current_group = None\n",
        "        current_columns = []\n",
        "    elif current_group and typ.startswith(\"likert5\"):\n",
        "        # Append only Likert5 \"feature\" variables -> ignore any that start with 'c_'\n",
        "        if not col.startswith(\"c_\"):\n",
        "            current_columns.append(col)\n",
        "\n",
        "# Add last group if not already added\n",
        "if current_group and current_columns:\n",
        "    groups[current_group] = current_columns\n",
        "\n",
        "# Display the flattened structure\n",
        "for g, cols in groups.items():\n",
        "    print(f\"{g}: {cols}\")"
      ],
      "metadata": {
        "id": "YZ1kHeF7Y4V6"
      },
      "id": "YZ1kHeF7Y4V6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Histograms for likert feature variables by groups"
      ],
      "metadata": {
        "id": "TLb62GJHZlnZ"
      },
      "id": "TLb62GJHZlnZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Likert5 textual labels for legend\n",
        "LIKERT_5_LABELS = [\"DK/NA\", \"1\", \"2\", \"3\", \"4\", \"5\"]\n",
        "\n",
        "# Likert5 colors: DK/NA gray, 1 red → 5 green\n",
        "LIKERT_5_COLORS = {\n",
        "    0: \"#e6e6e6\",  # DK/NA gray\n",
        "    1: \"#d73027\",  # red\n",
        "    2: \"#fc8d59\",  # orange\n",
        "    3: \"#fee08b\",  # amber\n",
        "    4: \"#91cf60\",  # lime green\n",
        "    5: \"#1a9850\",  # green\n",
        "}\n",
        "\n",
        "GEN_TITLE_MAP = {\n",
        "    \"gen_funct\": \"Functionality\",\n",
        "    \"gen_algo\": \"Algorithm\",\n",
        "    \"gen_lang\": \"Programming Language\",\n",
        "    \"gen_hw\": \"Quantum Backend\",\n",
        "    \"gen_integ\": \"Integration Model\"\n",
        "}\n",
        "\n",
        "def plot_likert5_group_fixed_height(df, group_name, columns, bar_height_cm=0.6):\n",
        "    \"\"\"\n",
        "    Plot 100% stacked horizontal bars for Likert5 variables,\n",
        "    with fixed bar height (in cm) for all plots, no vertical gaps,\n",
        "    and fully stacked.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        Processed DataFrame with numeric Likert5 columns\n",
        "    group_name : str\n",
        "        Name of the group\n",
        "    columns : list\n",
        "        List of Likert5 columns\n",
        "    bar_height_cm : float\n",
        "        Desired bar thickness in centimeters\n",
        "    \"\"\"\n",
        "    if not columns:\n",
        "        return\n",
        "\n",
        "    df_group = df[columns]\n",
        "\n",
        "    # Compute relative frequencies (percentages)\n",
        "    df_percent = df_group.apply(lambda x: x.value_counts(normalize=True).sort_index()).fillna(0)\n",
        "    df_percent = df_percent.T.iloc[::-1]  # reverse to have first variable at top\n",
        "\n",
        "    n_vars = len(columns)\n",
        "\n",
        "    # Convert bar height in cm to figure units\n",
        "    dpi = 100  # figure dpi\n",
        "    bar_height_inch = bar_height_cm / 2.54  # cm to inches\n",
        "    fig_height = n_vars * bar_height_inch  # total figure height\n",
        "    fig, ax = plt.subplots(figsize=(10, fig_height), dpi=dpi)\n",
        "\n",
        "    # Plot stacked bars manually\n",
        "    bottoms = np.zeros(n_vars)\n",
        "    for val in sorted(df_percent.columns):\n",
        "        ax.barh(\n",
        "            y=np.arange(n_vars),\n",
        "            width=df_percent[val],\n",
        "            left=bottoms,\n",
        "            color=LIKERT_5_COLORS.get(val, \"#cccccc\"),\n",
        "            height=1.0\n",
        "        )\n",
        "        bottoms += df_percent[val].values\n",
        "\n",
        "    # Add horizontal separator lines\n",
        "    for y in range(n_vars):\n",
        "        ax.hlines(y + 0.5, xmin=0, xmax=1, color='white', linestyle='-', linewidth=1)\n",
        "\n",
        "    # Set limits and labels\n",
        "    ax.set_ylim(-0.5, n_vars - 0.5)\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_yticks(np.arange(0, n_vars, 1))\n",
        "    ax.set_yticklabels(df_percent.index)\n",
        "    # ax.invert_yaxis()  # first variable at top\n",
        "\n",
        "    # ax.set_xlabel(\"Proportion\")\n",
        "    # ax.set_ylabel(\"Variables\")\n",
        "\n",
        "    # Map raw group_name to natural-language title if available\n",
        "    plot_title = GEN_TITLE_MAP.get(group_name, group_name)\n",
        "    ax.set_title(f\"Likert5 Responses Distribution - {plot_title}\")\n",
        "\n",
        "    # Vertical reference line at 50%\n",
        "    ax.axvline(0.5, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
        "\n",
        "    # Legend below plot, horizontal\n",
        "    handles = [plt.Rectangle((0,0),1,1,color=LIKERT_5_COLORS[v]) for v in sorted(df_percent.columns)]\n",
        "    ax.legend(handles, LIKERT_5_LABELS, title=\"Response\", loc='upper center',\n",
        "              bbox_to_anchor=(0.5, -0.15), ncol=len(LIKERT_5_LABELS))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "for group_name, cols in groups.items():\n",
        "    plot_likert5_group_fixed_height(df, group_name, cols, bar_height_cm=0.7)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SjqRrUbPbGoR"
      },
      "id": "SjqRrUbPbGoR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### features or intra-constraints without possitive valoration\n",
        "\n",
        "**Statistical screening rationale (one-sided binomial test)**\n",
        "\n",
        "**Goal.** For each Likert-5 item we quantify whether the item is *positively valued by users*. We define a response as *positive* if it is **4 or 5** on a 1–5 Likert scale. Let \\(p\\) be the true proportion of positive responses.\n",
        "\n",
        "**Statistic.** We compute the empirical **positive rate**:\n",
        "\\[\n",
        "\\hat{p} = \\frac{\\#\\{ \\text{responses } \\ge 4 \\}}{\\#\\{\\text{valid responses}\\}}.\n",
        "\\]\n",
        "\n",
        "**Hypothesis test (one-sided).**\n",
        "- Null hypothesis \\(H_0\\): \\(p = 0.50\\) (the positive rate equals 50%).\n",
        "- Alternative \\(H_1\\): \\(p < 0.50\\) (the positive rate is **below** 50%).\n",
        "\n",
        "We run a **one-sided binomial test** with \\(k\\) = number of positive responses (4–5) and \\(n\\) = number of valid responses. The test returns a **p-value**:\n",
        "- Small p-values (e.g., \\(p < 0.05\\)) provide evidence that the positive rate is **statistically below** 50%.\n",
        "\n",
        "**Decision rule reported.**\n",
        "- `valid_for_users = True` if \\($\\hat{p} \\ge 0.50$).\n",
        "- `valid_for_users = False` if \\($\\hat{p} < 0.50\\$).\n",
        "- We also report `significant_below_50` to indicate whether the test found \\(\\hat{p}\\) **significantly** below 50% at \\(\\alpha=0.05\\).\n",
        "\n",
        "**Interpretation.**\n",
        "- Items with `valid_for_users = False` are *not positively valued* according to the 50% criterion.\n",
        "- If, in addition, `significant_below_50 = True`, the shortfall is statistically supported (one-sided binomial test).\n",
        "\n",
        "**Notes.**\n",
        "- Missing values and values outside {1,2,3,4,5} are ignored.\n",
        "- You may adjust the threshold (e.g., 0.60) via `positive_threshold`, and the test level via `alpha`.\n",
        "- If many items are tested, consider multiple-comparison corrections for formal inference in a confirmatory analysis."
      ],
      "metadata": {
        "id": "QpoxCfrmoEIz"
      },
      "id": "QpoxCfrmoEIz"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Screening of \"positive for users\" within pre-defined groups (from DATA_DICTIONARY) ---\n",
        "# All code and comments in English.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import binomtest\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "def _likert5_positive_rate(series: pd.Series):\n",
        "    \"\"\"\n",
        "    Compute counts and rate for Likert-5 where positive is {4,5}.\n",
        "    Ignores NaN and values outside {1,2,3,4,5}.\n",
        "    Returns: n_total, n_pos, pos_rate (float in [0,1]).\n",
        "    \"\"\"\n",
        "    s = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
        "    s = s[s.isin([1,2,3,4,5])]\n",
        "    n = len(s)\n",
        "    if n == 0:\n",
        "        return 0, 0, np.nan\n",
        "    n_pos = (s >= 4).sum()\n",
        "    return n, n_pos, n_pos / n\n",
        "\n",
        "\n",
        "def screen_valid_for_users_by_groups(df: pd.DataFrame,\n",
        "                                     groups: \"OrderedDict[str, list[str]]\",\n",
        "                                     alpha: float = 0.05,\n",
        "                                     positive_threshold: float = 0.50,\n",
        "                                     export_csv: bool = False):\n",
        "    \"\"\"\n",
        "    For each group (as already discovered from DATA_DICTIONARY), compute:\n",
        "      - positive_rate_4_5     (statistic)\n",
        "      - one-sided binomial test p-value for H0: p = positive_threshold vs H1: p < threshold\n",
        "      - decision valid_for_users: positive_rate_4_5 >= positive_threshold\n",
        "      - significant_below_50: p-value < alpha (evidence of being below the threshold)\n",
        "\n",
        "    Prints a table per group and returns a combined DataFrame with a 'group' column.\n",
        "    \"\"\"\n",
        "    combined_rows = []\n",
        "\n",
        "    # Iterate groups in the provided order (OrderedDict preserves your order)\n",
        "    for gname, cols in groups.items():\n",
        "        rows = []\n",
        "        for col in cols:\n",
        "            # Defensive: skip columns not present in df\n",
        "            if col not in df.columns:\n",
        "                rows.append({\n",
        "                    \"group\": gname,\n",
        "                    \"variable\": col,\n",
        "                    \"n\": 0,\n",
        "                    \"positive_rate_4_5\": np.nan,\n",
        "                    f\"p_value_pos_lt_{int(positive_threshold*100)}%\": np.nan,\n",
        "                    \"valid_for_users\": np.nan,\n",
        "                    \"significant_below_50\": np.nan,\n",
        "                    \"note\": \"column not in df\"\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            n, n_pos, pos_rate = _likert5_positive_rate(df[col])\n",
        "\n",
        "            # Binomial one-sided test (only if we have data)\n",
        "            if n > 0 and not np.isnan(pos_rate):\n",
        "                pval = binomtest(k=n_pos, n=n, p=positive_threshold, alternative=\"less\").pvalue\n",
        "            else:\n",
        "                pval = np.nan\n",
        "\n",
        "            valid_for_users = (pos_rate >= positive_threshold) if not np.isnan(pos_rate) else np.nan\n",
        "            significant_below = (pval < alpha) if not np.isnan(pval) else np.nan\n",
        "\n",
        "            rows.append({\n",
        "                \"group\": gname,\n",
        "                \"variable\": col,\n",
        "                \"n\": n,\n",
        "                \"positive_rate_4_5\": pos_rate,\n",
        "                f\"p_value_pos_lt_{int(positive_threshold*100)}%\": pval,\n",
        "                \"valid_for_users\": valid_for_users,\n",
        "                \"significant_below_50\": significant_below\n",
        "            })\n",
        "\n",
        "        group_df = pd.DataFrame(rows)\n",
        "\n",
        "        # Pretty display (percentage as 0–100 with one decimal)\n",
        "        to_show = group_df.copy()\n",
        "        to_show[\"positive_rate_4_5\"] = (to_show[\"positive_rate_4_5\"] * 100).round(1)\n",
        "\n",
        "        display(Markdown(f\"### Group: `{gname}`\"))\n",
        "        display(\n",
        "            to_show[[\n",
        "                \"variable\",\n",
        "                \"n\",\n",
        "                \"positive_rate_4_5\",\n",
        "                f\"p_value_pos_lt_{int(positive_threshold*100)}%\",\n",
        "                \"valid_for_users\",\n",
        "                \"significant_below_50\"\n",
        "            ]].reset_index(drop=True)\n",
        "        )\n",
        "\n",
        "        if export_csv:\n",
        "            safe_gname = \"\".join(ch if ch.isalnum() or ch in \"_-\" else \"_\" for ch in gname)\n",
        "            to_show.to_csv(f\"valid_for_users_screen_{safe_gname}.csv\", index=False)\n",
        "\n",
        "        combined_rows.append(group_df)\n",
        "\n",
        "    # Return combined tidy DataFrame\n",
        "    if combined_rows:\n",
        "        combined = pd.concat(combined_rows, ignore_index=True)\n",
        "        combined_display = combined.copy()\n",
        "        combined_display[\"positive_rate_4_5\"] = (combined_display[\"positive_rate_4_5\"] * 100).round(1)\n",
        "        return combined, combined_display\n",
        "    else:\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# USAGE (runs immediately):\n",
        "# ---------------------------\n",
        "# Assumes `df` and `groups` exist per your snippet above.\n",
        "combined_raw, combined_pretty = screen_valid_for_users_by_groups(\n",
        "    df=df,\n",
        "    groups=groups,\n",
        "    alpha=0.05,\n",
        "    positive_threshold=0.50,\n",
        "    export_csv=False  # set True to save per-group CSVs\n",
        ")\n",
        "\n",
        "# Optionally inspect or save the combined results:\n",
        "# display(combined_pretty.sort_values([\"group\", \"positive_rate_4_5\"]))\n",
        "# combined_pretty.to_csv(\"valid_for_users_screen_ALL_GROUPS.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "gwtlEIeqoq3d"
      },
      "id": "gwtlEIeqoq3d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### features or intra-constraints without possitive valoration (without begginers)"
      ],
      "metadata": {
        "id": "ogX_pP_oaai1"
      },
      "id": "ogX_pP_oaai1"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Screening \"valid for users\" restricted by experience level (column: exp_q) ---\n",
        "# All code and comments in English.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import binomtest\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# We reuse _likert5_positive_rate from your previous cell.\n",
        "# If it is not in scope for some reason, uncomment this definition:\n",
        "# def _likert5_positive_rate(series: pd.Series):\n",
        "#     s = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
        "#     s = s[s.isin([1,2,3,4,5])]\n",
        "#     n = len(s)\n",
        "#     if n == 0:\n",
        "#         return 0, 0, np.nan\n",
        "#     n_pos = (s >= 4).sum()\n",
        "#     return n, n_pos, n_pos / n\n",
        "\n",
        "def screen_valid_for_users_by_groups_with_experience(df: pd.DataFrame,\n",
        "                                                     groups: \"OrderedDict[str, list[str]]\",\n",
        "                                                     exp_col: str = \"exp_q\",\n",
        "                                                     include_levels=None,\n",
        "                                                     alpha: float = 0.05,\n",
        "                                                     positive_threshold: float = 0.50,\n",
        "                                                     export_csv: bool = False,\n",
        "                                                     title_suffix: str = \"\"):\n",
        "    \"\"\"\n",
        "    Run the same screening as 'screen_valid_for_users_by_groups' but on a filtered subset\n",
        "    of respondents according to the experience column `exp_col`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Full survey dataframe.\n",
        "    groups : OrderedDict[str, list[str]]\n",
        "        Groups and their Likert-5 variables (features only).\n",
        "    exp_col : str\n",
        "        Column holding respondent experience text labels (e.g., \"exp_q\").\n",
        "    include_levels : list[str] or None\n",
        "        Which textual levels to keep. If None, defaults to excluding only \"Beginner (0-1 years)\"\n",
        "        and keeping Intermediate, Advanced, and Expert.\n",
        "        Examples:\n",
        "          - None (default): keep non-beginners only.\n",
        "          - [\"Expert (7+ years)\"]: keep experts only.\n",
        "          - [\"Intermediate (1-3 years)\", \"Advanced (3-5 years)\", \"Expert (7+ years)\"]: explicit non-beginners.\n",
        "    alpha : float\n",
        "        Significance level for the one-sided binomial test.\n",
        "    positive_threshold : float\n",
        "        Threshold for the positive rate (>= 4 on Likert-5) to declare valid_for_users.\n",
        "    export_csv : bool\n",
        "        If True, saves per-group CSVs of the pretty table.\n",
        "    title_suffix : str\n",
        "        Optional string appended to the per-group title to indicate the filter used.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    combined_raw : pd.DataFrame\n",
        "        Tidy numeric results across all groups (subset-filtered).\n",
        "    combined_display : pd.DataFrame\n",
        "        Human-friendly copy with percentages in [0,100].\n",
        "    \"\"\"\n",
        "    # Define canonical labels (as per your provided examples)\n",
        "    NON_BEGINNER_DEFAULT = [\n",
        "        \"Intermediate (1-3 years)\",\n",
        "        \"Advanced (3-5 years)\",\n",
        "        \"Expert (7+ years)\"\n",
        "    ]\n",
        "\n",
        "    if include_levels is None:\n",
        "        include_levels = NON_BEGINNER_DEFAULT\n",
        "\n",
        "    # Filter dataframe by experience\n",
        "    if exp_col not in df.columns:\n",
        "        raise KeyError(f\"Column '{exp_col}' not found in df.\")\n",
        "\n",
        "    df_filt = df[df[exp_col].isin(include_levels)].copy()\n",
        "\n",
        "    # Report the filter being used\n",
        "    display(Markdown(\n",
        "        f\"**Experience filter applied** on `{exp_col}`: {', '.join(include_levels)}  \"\n",
        "        f\"(n = {len(df_filt)})\"\n",
        "    ))\n",
        "\n",
        "    combined_rows = []\n",
        "\n",
        "    for gname, cols in groups.items():\n",
        "        rows = []\n",
        "        for col in cols:\n",
        "            if col not in df_filt.columns:\n",
        "                rows.append({\n",
        "                    \"group\": gname,\n",
        "                    \"variable\": col,\n",
        "                    \"n\": 0,\n",
        "                    \"positive_rate_4_5\": np.nan,\n",
        "                    f\"p_value_pos_lt_{int(positive_threshold*100)}%\": np.nan,\n",
        "                    \"valid_for_users\": np.nan,\n",
        "                    \"significant_below_50\": np.nan,\n",
        "                    \"note\": \"column not in filtered df\"\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            n, n_pos, pos_rate = _likert5_positive_rate(df_filt[col])\n",
        "\n",
        "            if n > 0 and not np.isnan(pos_rate):\n",
        "                pval = binomtest(k=n_pos, n=n, p=positive_threshold, alternative=\"less\").pvalue\n",
        "            else:\n",
        "                pval = np.nan\n",
        "\n",
        "            valid_for_users = (pos_rate >= positive_threshold) if not np.isnan(pos_rate) else np.nan\n",
        "            significant_below = (pval < alpha) if not np.isnan(pval) else np.nan\n",
        "\n",
        "            rows.append({\n",
        "                \"group\": gname,\n",
        "                \"variable\": col,\n",
        "                \"n\": n,\n",
        "                \"positive_rate_4_5\": pos_rate,\n",
        "                f\"p_value_pos_lt_{int(positive_threshold*100)}%\": pval,\n",
        "                \"valid_for_users\": valid_for_users,\n",
        "                \"significant_below_50\": significant_below\n",
        "            })\n",
        "\n",
        "        group_df = pd.DataFrame(rows)\n",
        "\n",
        "        # Pretty display (percentage as 0–100 with one decimal)\n",
        "        to_show = group_df.copy()\n",
        "        to_show[\"positive_rate_4_5\"] = (to_show[\"positive_rate_4_5\"] * 100).round(1)\n",
        "\n",
        "        subtitle = title_suffix.strip()\n",
        "        hdr = f\"### Group: `{gname}`\"\n",
        "        if subtitle:\n",
        "            hdr += f\" — {subtitle}\"\n",
        "\n",
        "        display(Markdown(hdr))\n",
        "        display(\n",
        "            to_show[[\n",
        "                \"variable\",\n",
        "                \"n\",\n",
        "                \"positive_rate_4_5\",\n",
        "                f\"p_value_pos_lt_{int(positive_threshold*100)}%\",\n",
        "                \"valid_for_users\",\n",
        "                \"significant_below_50\"\n",
        "            ]].reset_index(drop=True)\n",
        "        )\n",
        "\n",
        "        if export_csv:\n",
        "            def _safe(s): return \"\".join(ch if ch.isalnum() or ch in \"_-\" else \"_\" for ch in str(s))\n",
        "            safe_gname = _safe(gname)\n",
        "            safe_suffix = _safe(subtitle) if subtitle else \"filtered\"\n",
        "            to_show.to_csv(f\"valid_for_users_screen_{safe_gname}_{safe_suffix}.csv\", index=False)\n",
        "\n",
        "        combined_rows.append(group_df)\n",
        "\n",
        "    if combined_rows:\n",
        "        combined = pd.concat(combined_rows, ignore_index=True)\n",
        "        combined_display = combined.copy()\n",
        "        combined_display[\"positive_rate_4_5\"] = (combined_display[\"positive_rate_4_5\"] * 100).round(1)\n",
        "        return combined, combined_display\n",
        "    else:\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# USAGE EXAMPLES\n",
        "# ---------------------------\n",
        "\n",
        "# A) Non-beginners only (default): Intermediate + Advanced + Expert\n",
        "combined_raw_nb, combined_pretty_nb = screen_valid_for_users_by_groups_with_experience(\n",
        "    df=df,\n",
        "    groups=groups,\n",
        "    exp_col=\"exp_q\",\n",
        "    include_levels=None,                   # None -> keeps Intermediate, Advanced, Expert\n",
        "    alpha=0.05,\n",
        "    positive_threshold=0.50,\n",
        "    export_csv=False,\n",
        "    title_suffix=\"non-beginners only\"\n",
        ")\n",
        "\n",
        "# B) Experts only\n",
        "combined_raw_expert, combined_pretty_expert = screen_valid_for_users_by_groups_with_experience(\n",
        "    df=df,\n",
        "    groups=groups,\n",
        "    exp_col=\"exp_q\",\n",
        "    include_levels=[\"Expert (7+ years)\"],  # keep only Experts\n",
        "    alpha=0.05,\n",
        "    positive_threshold=0.50,\n",
        "    export_csv=False,\n",
        "    title_suffix=\"experts only\"\n",
        ")\n",
        "\n",
        "# C) Intermediate + Advanced (exclude both Beginners and Experts), if ever needed:\n",
        "# combined_raw_ia, combined_pretty_ia = screen_valid_for_users_by_groups_with_experience(\n",
        "#     df=df,\n",
        "#     groups=groups,\n",
        "#     exp_col=\"exp_q\",\n",
        "#     include_levels=[\"Intermediate (1-3 years)\", \"Advanced (3-5 years)\"],\n",
        "#     alpha=0.05,\n",
        "#     positive_threshold=0.50,\n",
        "#     export_csv=False,\n",
        "#     title_suffix=\"intermediate & advanced\"\n",
        "# )"
      ],
      "metadata": {
        "collapsed": true,
        "id": "K5JN2nXqZGQx"
      },
      "id": "K5JN2nXqZGQx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**7.** Reliability and Stability Analysis of Expert Judgements\n",
        "\n",
        "This section evaluates the reliability, agreement, and statistical robustness of the experts’ assessments on the relevance of the proposed features and constraints of the hybrid quantum–classical Feature Model (FM). Because the study is exploratory and the expert panel is heterogeneous in background, we do not rely on internal-consistency metrics; instead, we focus on inter-expert agreement and item-level evidence of support. The analysis proceeds in three main steps:\n",
        "\n",
        "---\n",
        "\n",
        "**1. Inter-Rater Reliability (IRR): Fleiss’ Kappa and Krippendorff’s Alpha**\n",
        "\n",
        "We compute two standard multi-rater agreement statistics:\n",
        "\n",
        "- **Fleiss’ Kappa** — measures categorical agreement beyond chance.  \n",
        "- **Krippendorff’s Alpha (ordinal)** — robust to missing data and suitable for Likert-scale judgements.\n",
        "\n",
        "These metrics are reported:\n",
        "- **globally**,  \n",
        "- **for each FM sub-tree**,  \n",
        "- **and for subgroups of experts** (e.g., experience in QC/SPL, role, QC areas, development phase),  \n",
        "including proper handling of multi-valued categorical variables (e.g., multiple phases or QC areas selected).\n",
        "\n",
        "Given the diversity of expertise, low agreement is expected; the purpose is to quantify differences and identify domains where disagreement is systematic.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Item-Level Support: Positive-Response Rate and I-CVI**\n",
        "\n",
        "For each proposed feature and constraint, we compute:\n",
        "\n",
        "- the **proportion of positive responses** (ratings ≥ 4),  \n",
        "- the **Item Content Validity Index (I-CVI)**, identical to the above and widely used in expert-judgement studies.\n",
        "\n",
        "Because this is an exploratory evaluation of a conceptual model, we adopt inclusive interpretive thresholds:\n",
        "\n",
        "- **Accepted**: point estimate ≥ 0.65  \n",
        "- **Limited evidence**: point estimate in [0.55, 0.65) or high uncertainty  \n",
        "- **Questionable**: CI_high < 0.55  \n",
        "\n",
        "These values quantify the extent to which experts consider each FM element relevant.\n",
        "\n",
        "---\n",
        "\n",
        "**3. Bootstrap Stability Analysis (1,000 resamples)**\n",
        "\n",
        "To assess the robustness of the estimated support for each item, we perform a **non-parametric bootstrap**:\n",
        "\n",
        "- resampling respondents with replacement (B = 1,000),  \n",
        "- recomputing the proportion of positive responses each time,  \n",
        "- and obtaining a **95% bootstrap confidence interval (CI)**.\n",
        "\n",
        "This analysis reveals whether an item’s support is:\n",
        "- **stable** (narrow CI well above 0.65),  \n",
        "- **borderline or uncertain** (CI crosses 0.55–0.65),  \n",
        "- **consistently weak** (CI entirely below 0.55).\n",
        "\n",
        "This step is essential to determine whether the observed support is robust or driven by sampling variability.\n",
        "\n",
        "---\n",
        "\n",
        "**4. Visual Summary: Traffic-Light Forest Plots**\n",
        "\n",
        "For each FM group, we present a compact forest plot showing:\n",
        "- the **point estimate** of expert support,  \n",
        "- the **95% bootstrap CI**,  \n",
        "- and a **traffic-light classification**:\n",
        "  - **green** (accepted),  \n",
        "  - **amber** (limited evidence),  \n",
        "  - **red** (questionable).\n",
        "\n",
        "Items are ordered from strongest to weakest support, and plots are formatted to maintain consistent data-area width regardless of label length.\n",
        "\n",
        "---\n",
        "\n",
        "**Outcome**\n",
        "\n",
        "Together, these analyses provide a structured, statistically grounded assessment of the experts’ evaluations. They identify:\n",
        "- features with robust evidence of relevance,  \n",
        "- features requiring refinement or clearer definition,  \n",
        "- and features with insufficient support.\n",
        "\n",
        "This reliability and stability assessment directly informs the consolidation and refinement of the proposed Feature Model.\n"
      ],
      "metadata": {
        "id": "hMiQK1cCiAwd"
      },
      "id": "hMiQK1cCiAwd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Descriptive statistics per item (Likert-based validation of FM features)\n",
        "\n",
        "In this section we summarise each survey item that evaluates **the relevance of a feature or constraint in the Feature Model**.\n",
        "\n",
        "For each Likert item we report:\n",
        "\n",
        "- `N_total`: number of non-missing responses (including DK/NA).  \n",
        "- `N_valid`: number of responses excluding DK/NA (i.e., values > 0).  \n",
        "- `DKNA_%`: percentage of “Don’t know / Not applicable” answers (coded as 0).  \n",
        "- `mean`, `median`, `sd`: descriptive statistics on the **valid** responses (1–5 or 1–3).  \n",
        "- `%neg`: percentage of negative answers (1–2 in Likert-5; 1 in Likert-3).  \n",
        "- `%neutral`: percentage of neutral answers (3 in Likert-5; 2 in Likert-3).  \n",
        "- `%pos`: percentage of positive answers (4–5 in Likert-5; 3 in Likert-3).\n",
        "\n",
        "We treat DK/NA as “informative non-endorsement”: it is reported in `DKNA_%` but excluded from the computation of mean/median and from the negative/neutral/positive percentages.\n",
        "\n",
        "### * Descriptive statistics grouped by FM dimension\n",
        "\n",
        "Using the grouped structure automatically derived from `DATA_DICTIONARY`, we present below the descriptive statistics for all Likert-5 items (features and constraints excluded) grouped under:\n",
        "\n",
        "- `gen_funct`: Functional capabilities  \n",
        "- `gen_algo`: Algorithmic paradigms  \n",
        "- `gen_lang`: Frameworks and languages  \n",
        "- `gen_hw`: Hardware, error models, control  \n",
        "- `gen_integ`: Integration and orchestration\n",
        "\n",
        "Each table reports:  \n",
        "`N_total`, `N_valid`, `DKNA_%`, `mean`, `median`, `sd`, `%neg`, `%neutral`, `%pos`.\n"
      ],
      "metadata": {
        "id": "DBuq4wfEurv0"
      },
      "id": "DBuq4wfEurv0"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Identify Likert-5 and Likert-3 columns from DATA_DICTIONARY\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "# All Likert-5 and Likert-3 variables present in df\n",
        "LIKERT5_COLS = [\n",
        "    col for col, typ in DATA_DICTIONARY.items()\n",
        "    if typ == \"likert5\" and col in df.columns\n",
        "]\n",
        "\n",
        "LIKERT3_COLS = [\n",
        "    col for col, typ in DATA_DICTIONARY.items()\n",
        "    if typ == \"likert3\" and col in df.columns\n",
        "]\n",
        "\n",
        "print(\"Total Likert-5 columns:\", len(LIKERT5_COLS))\n",
        "print(\"Total Likert-3 columns:\", len(LIKERT3_COLS))\n",
        "\n",
        "# Optional: restrict to FM-related features/constraints only\n",
        "# (you can comment this out if you prefer to include all Likert vars)\n",
        "FM_LIKERT5_COLS = [\n",
        "    c for c in LIKERT5_COLS\n",
        "    if c.startswith((\"f_\", \"c_\"))   # heuristic for features/constraints\n",
        "]\n",
        "\n",
        "FM_LIKERT3_COLS = [\n",
        "    c for c in LIKERT3_COLS\n",
        "    if c.startswith((\"gen_\", \"ccc_\", \"gen_\"))  # e.g., general questions about the FM\n",
        "]\n",
        "\n",
        "print(\"FM Likert-5 columns:\", len(FM_LIKERT5_COLS))\n",
        "print(\"FM Likert-3 columns:\", len(FM_LIKERT3_COLS))\n"
      ],
      "metadata": {
        "id": "CiUFSjSgrs6h"
      },
      "id": "CiUFSjSgrs6h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def describe_likert_items(\n",
        "    df: pd.DataFrame,\n",
        "    cols,\n",
        "    *,\n",
        "    positive_threshold: int,\n",
        "    neutral_value: int | None,\n",
        "    label: str = \"likert\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build descriptive statistics for Likert items.\n",
        "\n",
        "    Assumptions:\n",
        "    - DK/NA is coded as 0.\n",
        "    - Valid responses are >0.\n",
        "    - For Likert-5:\n",
        "        negative = {1, 2}, neutral = {3}, positive = {4, 5}\n",
        "    - For Likert-3:\n",
        "        negative = {1}, neutral = {2}, positive = {3}\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "\n",
        "    for col in cols:\n",
        "        if col not in df.columns:\n",
        "            continue\n",
        "\n",
        "        s = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "        # Total responses (including DK/NA)\n",
        "        total_non_missing = s.notna().sum()\n",
        "\n",
        "        # DK/NA\n",
        "        dkna_count = (s == 0).sum()\n",
        "\n",
        "        # Valid Likert responses\n",
        "        valid = s[s > 0].dropna()\n",
        "        n_valid = valid.shape[0]\n",
        "\n",
        "        if total_non_missing == 0:\n",
        "            # Nothing answered at all for this item\n",
        "            rows.append({\n",
        "                \"item\": col,\n",
        "                \"scale\": label,\n",
        "                \"N_total\": 0,\n",
        "                \"N_valid\": 0,\n",
        "                \"DKNA_%\": np.nan,\n",
        "                \"mean\": np.nan,\n",
        "                \"median\": np.nan,\n",
        "                \"sd\": np.nan,\n",
        "                \"%neg\": np.nan,\n",
        "                \"%neutral\": np.nan,\n",
        "                \"%pos\": np.nan,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Descriptive statistics on valid answers\n",
        "        if n_valid > 0:\n",
        "            mean = valid.mean()\n",
        "            median = valid.median()\n",
        "            sd = valid.std(ddof=1)\n",
        "        else:\n",
        "            mean = median = sd = np.nan\n",
        "\n",
        "        # Negative / neutral / positive counts (on valid responses)\n",
        "        if n_valid > 0:\n",
        "            # Negative\n",
        "            if neutral_value is not None:\n",
        "                neg_mask = (valid >= 1) & (valid <= neutral_value - 1)\n",
        "            else:\n",
        "                neg_mask = valid == 1  # very conservative default\n",
        "            neg_count = neg_mask.sum()\n",
        "\n",
        "            # Neutral\n",
        "            if neutral_value is not None:\n",
        "                neut_mask = (valid == neutral_value)\n",
        "                neut_count = neut_mask.sum()\n",
        "            else:\n",
        "                neut_count = 0\n",
        "\n",
        "            # Positive\n",
        "            pos_mask = (valid >= positive_threshold)\n",
        "            pos_count = pos_mask.sum()\n",
        "\n",
        "            # Percentages over valid responses\n",
        "            denom = float(n_valid)\n",
        "            neg_pct = 100.0 * neg_count / denom\n",
        "            neut_pct = 100.0 * neut_count / denom\n",
        "            pos_pct = 100.0 * pos_count / denom\n",
        "        else:\n",
        "            neg_pct = neut_pct = pos_pct = np.nan\n",
        "\n",
        "        dkna_pct = 100.0 * dkna_count / float(total_non_missing)\n",
        "\n",
        "        rows.append({\n",
        "            \"item\": col,\n",
        "            \"scale\": label,\n",
        "            \"N_total\": int(total_non_missing),\n",
        "            \"N_valid\": int(n_valid),\n",
        "            \"DKNA_%\": round(dkna_pct, 1),\n",
        "            \"mean\": round(mean, 2) if not np.isnan(mean) else np.nan,\n",
        "            \"median\": median if not np.isnan(median) else np.nan,\n",
        "            \"sd\": round(sd, 2) if not np.isnan(sd) else np.nan,\n",
        "            \"%neg\": round(neg_pct, 1) if not np.isnan(neg_pct) else np.nan,\n",
        "            \"%neutral\": round(neut_pct, 1) if not np.isnan(neut_pct) else np.nan,\n",
        "            \"%pos\": round(pos_pct, 1) if not np.isnan(pos_pct) else np.nan,\n",
        "        })\n",
        "\n",
        "    out = pd.DataFrame(rows)\n",
        "    # Optional: sort by descending %pos to see most-endorsed features first\n",
        "    if \"%pos\" in out.columns:\n",
        "        out = out.sort_values(by=\"%pos\", ascending=False)\n",
        "    return out.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "mERQUnhmu1-X"
      },
      "id": "mERQUnhmu1-X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Descriptive tables for the Feature Model items\n",
        "\n",
        "desc_fm_likert5 = describe_likert_items(\n",
        "    df,\n",
        "    FM_LIKERT5_COLS,\n",
        "    positive_threshold=4,   # 4–5 = agreement\n",
        "    neutral_value=3,\n",
        "    label=\"likert5\"\n",
        ")\n",
        "\n",
        "desc_fm_likert3 = describe_likert_items(\n",
        "    df,\n",
        "    FM_LIKERT3_COLS,\n",
        "    positive_threshold=3,   # 3 = Yes\n",
        "    neutral_value=2,        # 2 = Partially\n",
        "    label=\"likert3\"\n",
        ")\n",
        "\n",
        "print(\"=== Descriptives for FM Likert-5 items ===\")\n",
        "display(desc_fm_likert5)\n",
        "\n",
        "print(\"\\n=== Descriptives for FM Likert-3 items ===\")\n",
        "display(desc_fm_likert3)\n"
      ],
      "metadata": {
        "id": "myFEu6kIrljy"
      },
      "id": "myFEu6kIrljy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to show one group\n",
        "def show_group_table(desc_table, group_name, cols):\n",
        "    subset = desc_table[desc_table[\"item\"].isin(cols)]\n",
        "    print(f\"\\n=== {group_name} ({len(subset)} items) ===\")\n",
        "    display(subset.reset_index(drop=True))\n",
        "\n",
        "\n",
        "# Iterate over detected groups in the correct order\n",
        "for group_name, cols in groups.items():\n",
        "    show_group_table(desc_fm_likert5, group_name, cols)\n"
      ],
      "metadata": {
        "id": "A9U0Aoxwwros"
      },
      "id": "A9U0Aoxwwros",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Content Validity Index (CVI)\n",
        "\n",
        "To assess whether each feature of the Feature Model is considered *relevant* by the experts, we compute:\n",
        "\n",
        "- **I-CVI (Item-Level Content Validity Index):**  \n",
        "  Proportion of experts rating the item as *relevant*.  \n",
        "  For Likert-5 scales, relevance is defined as responses `4` or `5`.\n",
        "\n",
        "- **S-CVI/Ave (Scale-Level CVI):**  \n",
        "  Average I-CVI across all items within each FM dimension (`gen_funct`, `gen_algo`, `gen_lang`, `gen_hw`, `gen_integ`).\n",
        "\n",
        "Interpretation follows Lynn (1986):  \n",
        "- `I-CVI ≥ 0.78` → the item is considered valid by experts.  \n",
        "\n",
        "We also report:  \n",
        "- `N_valid`: number of raters with valid responses (excluding DK/NA).  \n",
        "- `%pos`: percentage of raters selecting `4` or `5`.  \n",
        "\n",
        "**Rationale:** “Twenty-six experts participated in the content validation. Given the exploratory nature of the study and the highly specialised domain (hybrid quantum–classical software), we followed Polit and Beck’s recommendations and adopted an I-CVI threshold of 0.70 for item acceptance, which is appropriate for expert-based conceptual validation with larger expert panels.\n",
        "\n",
        "Items with I-CVI ≥ 0.70 were considered as having high content validity. Items with 0.60 ≤ I-CVI < 0.70 were classified as having moderate validity and marked for revision. Items with I-CVI < 0.60 were considered to have low content validity and were candidates for exclusion from the final Feature Model.”\n"
      ],
      "metadata": {
        "id": "DsgI1fXsyFlz"
      },
      "id": "DsgI1fXsyFlz"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_icvi(\n",
        "    df,\n",
        "    cols,\n",
        "    positive_threshold=4,\n",
        "    accept_threshold=0.70,\n",
        "    review_threshold=0.60\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute I-CVI and positive response ratios for each Likert-5 item.\n",
        "\n",
        "    Classification:\n",
        "        - 'accepted'  : I-CVI >= accept_threshold\n",
        "        - 'review'    : review_threshold <= I-CVI < accept_threshold\n",
        "        - 'rejected'  : I-CVI < review_threshold\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "\n",
        "    for col in cols:\n",
        "        if col not in df.columns:\n",
        "            continue\n",
        "\n",
        "        s = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "        valid = s[s > 0]               # remove DK/NA\n",
        "        n_valid = valid.shape[0]\n",
        "\n",
        "        if n_valid == 0:\n",
        "            rows.append({\n",
        "                \"item\": col,\n",
        "                \"N_valid\": 0,\n",
        "                \"I-CVI\": np.nan,\n",
        "                \"%pos\": np.nan,\n",
        "                \"status\": \"no_data\"\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        pos = (valid >= positive_threshold).sum()\n",
        "        i_cvi = pos / n_valid\n",
        "        pct_pos = 100 * i_cvi\n",
        "\n",
        "        # Classification according to thresholds\n",
        "        if i_cvi >= accept_threshold:\n",
        "            status = \"accepted\"\n",
        "        elif i_cvi >= review_threshold:\n",
        "            status = \"review\"\n",
        "        else:\n",
        "            status = \"rejected\"\n",
        "\n",
        "        rows.append({\n",
        "            \"item\": col,\n",
        "            \"N_valid\": n_valid,\n",
        "            \"I-CVI\": round(i_cvi, 3),\n",
        "            \"%pos\": round(pct_pos, 1),\n",
        "            \"status\": status\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows).sort_values(by=\"I-CVI\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Compute I-CVI for all Likert-5 \"f_\" items grouped under `groups`\n",
        "icvi_results = {}\n",
        "for group_name, cols in groups.items():\n",
        "    icvi_results[group_name] = compute_icvi(df, cols)\n",
        "\n",
        "# Example output\n",
        "for group, table in icvi_results.items():\n",
        "    print(f\"\\n=== I-CVI for group: {group} ===\")\n",
        "    display(table)\n"
      ],
      "metadata": {
        "id": "eJM7m-GpyIGi"
      },
      "id": "eJM7m-GpyIGi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute S-CVI/Ave (average of the I-CVI values per group)\n",
        "s_cvi_results = []\n",
        "\n",
        "for group_name, table in icvi_results.items():\n",
        "    valid_icvi_values = table[\"I-CVI\"].dropna()\n",
        "    if len(valid_icvi_values) == 0:\n",
        "        s_cvi = np.nan\n",
        "    else:\n",
        "        s_cvi = valid_icvi_values.mean()\n",
        "\n",
        "    s_cvi_results.append({\n",
        "        \"group\": group_name,\n",
        "        \"num_items\": len(valid_icvi_values),\n",
        "        \"S-CVI/Ave\": round(s_cvi, 3)\n",
        "    })\n",
        "\n",
        "s_cvi_df = pd.DataFrame(s_cvi_results)\n",
        "\n",
        "print(\"=== S-CVI/Ave per FM group ===\")\n",
        "display(s_cvi_df.sort_values(by=\"S-CVI/Ave\", ascending=False))\n"
      ],
      "metadata": {
        "id": "yE37y5UjzA7a"
      },
      "id": "yE37y5UjzA7a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Inter-rater reliability (Fleiss’ Kappa and Krippendorff’s Alpha)\n",
        "\n",
        "Beyond content validity (CVI), we assess the **consistency of agreement among experts**.\n",
        "\n",
        "We recode Likert-5 responses into three ordered categories:\n",
        "\n",
        "- `0 = disagreement` (original values 1–2)\n",
        "- `1 = neutral` (original value 3)\n",
        "- `2 = agreement` (original values 4–5)\n",
        "\n",
        "Missing values and DK/NA (coded as 0) are treated as missing.\n",
        "\n",
        "On this recoded matrix we compute:\n",
        "\n",
        "- **Fleiss’ Kappa** (with multiple raters and varying numbers of valid responses per item) to quantify agreement beyond chance.\n",
        "- **Krippendorff’s Alpha (ordinal)**, which explicitly accounts for the ordered nature of the categories (disagreement–neutral–agreement) and handles missing data.\n",
        "\n",
        "We report these indices:\n",
        "\n",
        "- for all Feature Model items together, and  \n",
        "- per conceptual group (`gen_funct`, `gen_algo`, `gen_lang`, `gen_hw`, `gen_integ`).\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "- For Fleiss’ Kappa (Landis & Koch, 1977):\n",
        "  - 0.41–0.60 → moderate agreement  \n",
        "  - 0.61–0.80 → substantial agreement  \n",
        "  - 0.81–1.00 → almost perfect agreement\n",
        "\n",
        "- For Krippendorff’s Alpha (ordinal):\n",
        "  - ≥ 0.67 → acceptable for exploratory research  \n",
        "  - ≥ 0.80 → good for confirmatory purposes.\n",
        "\n"
      ],
      "metadata": {
        "id": "aUdb2DemHzqD"
      },
      "id": "aUdb2DemHzqD"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Collect all Feature Model Likert-5 columns from the 'groups' structure\n",
        "all_fm_cols = []\n",
        "for _, cols in groups.items():\n",
        "    all_fm_cols.extend(cols)\n",
        "\n",
        "# Keep only those that actually exist in df (defensive)\n",
        "all_fm_cols = [c for c in all_fm_cols if c in df.columns]\n",
        "\n",
        "print(\"Total FM Likert-5 items used for agreement analysis:\", len(all_fm_cols))\n",
        "\n",
        "# 2) Recode Likert-5 into 3-category agreement scale:\n",
        "#    0 = disagreement (1–2)\n",
        "#    1 = neutral      (3)\n",
        "#    2 = agreement    (4–5)\n",
        "#    0 or NaN in the original (DK/NA) => NaN here\n",
        "def recode_likert_to_agreement(s: pd.Series) -> pd.Series:\n",
        "    s_num = pd.to_numeric(s, errors=\"coerce\")\n",
        "    out = pd.Series(index=s_num.index, dtype=\"float\")\n",
        "\n",
        "    # DK/NA or invalid => NaN\n",
        "    out[:] = np.nan\n",
        "\n",
        "    # Disagreement: 1–2\n",
        "    out[(s_num == 1) | (s_num == 2)] = 0\n",
        "\n",
        "    # Neutral: 3\n",
        "    out[s_num == 3] = 1\n",
        "\n",
        "    # Agreement: 4–5\n",
        "    out[(s_num == 4) | (s_num == 5)] = 2\n",
        "\n",
        "    return out\n",
        "\n",
        "# Build the recoded DataFrame\n",
        "df_fm_agreement = pd.DataFrame(index=df.index)\n",
        "\n",
        "for col in all_fm_cols:\n",
        "    df_fm_agreement[col] = recode_likert_to_agreement(df[col])\n",
        "\n",
        "print(\"Shape of agreement matrix (raters × items):\", df_fm_agreement.shape)\n",
        "display(df_fm_agreement.head())\n"
      ],
      "metadata": {
        "id": "f0ngry90H1gX"
      },
      "id": "f0ngry90H1gX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "def fleiss_kappa_from_matrix(df_cat: pd.DataFrame) -> float:\n",
        "    \"\"\"\n",
        "    Compute Fleiss' Kappa for a ratings matrix with:\n",
        "        - rows = raters\n",
        "        - columns = items\n",
        "        - values = integer categories (0, 1, 2, ...), NaN = missing\n",
        "\n",
        "    This implementation allows varying numbers of valid ratings per item.\n",
        "    \"\"\"\n",
        "    values = df_cat.values\n",
        "    # Extract all observed categories\n",
        "    valid_values = values[~np.isnan(values)].astype(int)\n",
        "    if valid_values.size == 0:\n",
        "        return np.nan\n",
        "\n",
        "    categories = np.unique(valid_values)\n",
        "    k = len(categories)\n",
        "\n",
        "    # Mapping category value -> index 0..k-1\n",
        "    cat_to_idx = {c: i for i, c in enumerate(categories)}\n",
        "\n",
        "    P_i_list = []\n",
        "    total_ratings = 0\n",
        "    # Sum of counts per category across all items\n",
        "    n_ik_sum = np.zeros(k, dtype=float)\n",
        "\n",
        "    # Iterate over items (columns)\n",
        "    for j in range(values.shape[1]):\n",
        "        col = values[:, j]\n",
        "        v = col[~np.isnan(col)].astype(int)\n",
        "        n_i = len(v)\n",
        "        if n_i < 2:\n",
        "            # Not enough ratings for this item\n",
        "            continue\n",
        "\n",
        "        total_ratings += n_i\n",
        "\n",
        "        # Count per category\n",
        "        counts = np.zeros(k, dtype=float)\n",
        "        for val in v:\n",
        "            counts[cat_to_idx[val]] += 1\n",
        "\n",
        "        # Update global category sums\n",
        "        n_ik_sum += counts\n",
        "\n",
        "        # Agreement for item i\n",
        "        P_i = (np.sum(counts ** 2) - n_i) / (n_i * (n_i - 1))\n",
        "        P_i_list.append(P_i)\n",
        "\n",
        "    if len(P_i_list) == 0 or total_ratings == 0:\n",
        "        return np.nan\n",
        "\n",
        "    P_bar = np.mean(P_i_list)\n",
        "\n",
        "    # Overall category proportions\n",
        "    p_k = n_ik_sum / total_ratings\n",
        "    P_e = np.sum(p_k ** 2)\n",
        "\n",
        "    if P_e == 1.0:\n",
        "        # No variability in ratings\n",
        "        return np.nan\n",
        "\n",
        "    kappa = (P_bar - P_e) / (1.0 - P_e)\n",
        "    return float(kappa)\n",
        "\n",
        "\n",
        "def krippendorff_alpha_ordinal(df_cat: pd.DataFrame, n_levels: int = 3) -> float:\n",
        "    \"\"\"\n",
        "    Compute Krippendorff's Alpha for ordinal data from a ratings matrix:\n",
        "        - rows = raters\n",
        "        - columns = items\n",
        "        - values = integer categories in {0, 1, ..., n_levels-1}, NaN = missing\n",
        "\n",
        "    Uses squared distance scaled to [0, 1] between ordinal categories.\n",
        "    \"\"\"\n",
        "    values = df_cat.values\n",
        "    valid_flat = values[~np.isnan(values)].astype(int)\n",
        "    if valid_flat.size < 2:\n",
        "        return np.nan\n",
        "\n",
        "    # Distance matrix for ordinal categories: normalized squared difference\n",
        "    idx = np.arange(n_levels)\n",
        "    D = ((idx[:, None] - idx[None, :]) / (n_levels - 1)) ** 2  # shape (n_levels, n_levels)\n",
        "\n",
        "    # Marginal distribution of categories (for expected disagreement)\n",
        "    freqs = np.array([(valid_flat == k).sum() for k in range(n_levels)], dtype=float)\n",
        "    total = freqs.sum()\n",
        "    if total == 0:\n",
        "        return np.nan\n",
        "\n",
        "    p = freqs / total\n",
        "\n",
        "    # Expected disagreement: average squared distance between two independent draws\n",
        "    De = 0.0\n",
        "    for k in range(n_levels):\n",
        "        for l in range(n_levels):\n",
        "            De += p[k] * p[l] * D[k, l]\n",
        "\n",
        "    if De == 0:\n",
        "        return np.nan\n",
        "\n",
        "    # Observed disagreement: average squared distance between all pairs of ratings on the same item\n",
        "    sum_Do = 0.0\n",
        "    count_pairs = 0\n",
        "\n",
        "    n_raters, n_items = values.shape\n",
        "    for j in range(n_items):\n",
        "        col = values[:, j]\n",
        "        v = col[~np.isnan(col)].astype(int)\n",
        "        if v.size < 2:\n",
        "            continue\n",
        "\n",
        "        # All unordered pairs of ratings for this item\n",
        "        for a, b in itertools.combinations(v, 2):\n",
        "            sum_Do += D[a, b]\n",
        "            count_pairs += 1\n",
        "\n",
        "    if count_pairs == 0:\n",
        "        return np.nan\n",
        "\n",
        "    Do = sum_Do / count_pairs\n",
        "    alpha = 1.0 - (Do / De)\n",
        "    return float(alpha)\n"
      ],
      "metadata": {
        "id": "HjKt7pf5Idyp"
      },
      "id": "HjKt7pf5Idyp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Global inter-rater reliability for all FM items\n",
        "global_kappa = fleiss_kappa_from_matrix(df_fm_agreement)\n",
        "global_alpha = krippendorff_alpha_ordinal(df_fm_agreement, n_levels=3)\n",
        "\n",
        "print(\"=== Global inter-rater reliability for all FM items ===\")\n",
        "print(f\"Fleiss' Kappa      : {global_kappa:.3f}\" if not np.isnan(global_kappa) else \"Fleiss' Kappa      : NA\")\n",
        "print(f\"Krippendorff Alpha : {global_alpha:.3f}\" if not np.isnan(global_alpha) else \"Krippendorff Alpha : NA\")\n",
        "\n",
        "\n",
        "# 2) Inter-rater reliability per conceptual group\n",
        "group_irr_rows = []\n",
        "\n",
        "for group_name, cols in groups.items():\n",
        "    cols_in_df = [c for c in cols if c in df_fm_agreement.columns]\n",
        "    if not cols_in_df:\n",
        "        continue\n",
        "\n",
        "    submatrix = df_fm_agreement[cols_in_df]\n",
        "\n",
        "    kappa_g = fleiss_kappa_from_matrix(submatrix)\n",
        "    alpha_g = krippendorff_alpha_ordinal(submatrix, n_levels=3)\n",
        "\n",
        "    group_irr_rows.append({\n",
        "        \"group\": group_name,\n",
        "        \"num_items\": len(cols_in_df),\n",
        "        \"Fleiss_Kappa\": round(kappa_g, 3) if not np.isnan(kappa_g) else np.nan,\n",
        "        \"Krippendorff_Alpha\": round(alpha_g, 3) if not np.isnan(alpha_g) else np.nan,\n",
        "    })\n",
        "\n",
        "group_irr_df = pd.DataFrame(group_irr_rows)\n",
        "\n",
        "print(\"\\n=== Inter-rater reliability per FM group ===\")\n",
        "display(group_irr_df)\n"
      ],
      "metadata": {
        "id": "RZII6ZC3Iku4"
      },
      "id": "RZII6ZC3Iku4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inter-rater reliability within subgroups\n",
        "\n",
        "Given the heterogeneity of the expert panel, we compute inter-rater reliability\n",
        "(Fleiss’ Kappa and Krippendorff’s Alpha) for *subgroups* defined by categorical\n",
        "variables in the dataset (e.g., expertise level, domain experience, background).\n",
        "\n",
        "This allows us to quantify agreement *within* more homogeneous subsets of experts,\n",
        "which is more meaningful in exploratory multi-domain studies.\n"
      ],
      "metadata": {
        "id": "odViB2F3J2DI"
      },
      "id": "odViB2F3J2DI"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def compute_irr_for_subgroup(df_agree: pd.DataFrame,\n",
        "                             subgroup_mask: pd.Series,\n",
        "                             groups,\n",
        "                             n_levels: int = 3) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute Fleiss' Kappa and Krippendorff's Alpha for:\n",
        "        1) all FM items\n",
        "        2) each FM conceptual group (gen_funct, gen_algo, ...)\n",
        "    restricted to the raters that satisfy `subgroup_mask`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_agree : DataFrame\n",
        "        Ratings matrix (rows = raters, columns = FM items),\n",
        "        with integer categories {0,1,...,n_levels-1} and NaN = missing.\n",
        "    subgroup_mask : Series[bool]\n",
        "        Boolean mask selecting the subset of raters (same index as df_agree).\n",
        "    groups : OrderedDict\n",
        "        Mapping from group_name -> list of item names (columns in df_agree).\n",
        "    n_levels : int\n",
        "        Number of ordinal categories (3 in our case: 0,1,2).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    DataFrame with rows:\n",
        "        - group (ALL_ITEMS or group name)\n",
        "        - num_raters\n",
        "        - num_items\n",
        "        - Fleiss_Kappa\n",
        "        - Krippendorff_Alpha\n",
        "    \"\"\"\n",
        "\n",
        "    # Asegurarse de que el índice coincide\n",
        "    subgroup_mask = subgroup_mask.reindex(df_agree.index).fillna(False)\n",
        "\n",
        "    n_raters = int(subgroup_mask.sum())\n",
        "    if n_raters < 2:\n",
        "        # Demasiado pocos evaluadores para un análisis razonable\n",
        "        return pd.DataFrame([{\n",
        "            \"group\": \"ALL_ITEMS\",\n",
        "            \"num_raters\": n_raters,\n",
        "            \"num_items\": 0,\n",
        "            \"Fleiss_Kappa\": np.nan,\n",
        "            \"Krippendorff_Alpha\": np.nan\n",
        "        }])\n",
        "\n",
        "    # Filtrar matriz de acuerdo\n",
        "    sub_df = df_agree.loc[subgroup_mask]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # 1) Fiabilidad global (todos los ítems del FM)\n",
        "    kappa_global = fleiss_kappa_from_matrix(sub_df)\n",
        "    alpha_global = krippendorff_alpha_ordinal(sub_df, n_levels=n_levels)\n",
        "\n",
        "    results.append({\n",
        "        \"group\": \"ALL_ITEMS\",\n",
        "        \"num_raters\": n_raters,\n",
        "        \"num_items\": sub_df.shape[1],\n",
        "        \"Fleiss_Kappa\": round(kappa_global, 3) if not np.isnan(kappa_global) else np.nan,\n",
        "        \"Krippendorff_Alpha\": round(alpha_global, 3) if not np.isnan(alpha_global) else np.nan\n",
        "    })\n",
        "\n",
        "    # 2) Fiabilidad por subárbol conceptual del FM\n",
        "    for gname, cols in groups.items():\n",
        "        cols_in_df = [c for c in cols if c in sub_df.columns]\n",
        "        if not cols_in_df:\n",
        "            continue\n",
        "\n",
        "        sub_mat = sub_df[cols_in_df]\n",
        "\n",
        "        kappa_g = fleiss_kappa_from_matrix(sub_mat)\n",
        "        alpha_g = krippendorff_alpha_ordinal(sub_mat, n_levels=n_levels)\n",
        "\n",
        "        results.append({\n",
        "            \"group\": gname,\n",
        "            \"num_raters\": n_raters,\n",
        "            \"num_items\": len(cols_in_df),\n",
        "            \"Fleiss_Kappa\": round(kappa_g, 3) if not np.isnan(kappa_g) else np.nan,\n",
        "            \"Krippendorff_Alpha\": round(alpha_g, 3) if not np.isnan(alpha_g) else np.nan\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n"
      ],
      "metadata": {
        "id": "e_ebMTzrJ0Ru"
      },
      "id": "e_ebMTzrJ0Ru",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_unique_multicategories(df, column):\n",
        "    \"\"\"\n",
        "    Given a column with comma-separated multi-select categories,\n",
        "    extract the set of unique individual categories.\n",
        "    \"\"\"\n",
        "    if column not in df.columns:\n",
        "        return []\n",
        "\n",
        "    unique_cats = set()\n",
        "    for raw in df[column].dropna().astype(str):\n",
        "        parts = [p.strip() for p in raw.split(\",\") if p.strip()]\n",
        "        for p in parts:\n",
        "            unique_cats.add(p)\n",
        "\n",
        "    return sorted(unique_cats)\n",
        "\n",
        "\n",
        "def mask_for_multicategory(df, column, category):\n",
        "    \"\"\"\n",
        "    Build a boolean mask selecting rows where `category` appears\n",
        "    in the comma-separated list in df[column].\n",
        "\n",
        "    Works also when df[column] is of dtype 'category', by converting\n",
        "    it safely to object/str first.\n",
        "    \"\"\"\n",
        "    if column not in df.columns:\n",
        "        return pd.Series(False, index=df.index)\n",
        "\n",
        "    # Convert safely to plain strings (avoid categorical fillna issues)\n",
        "    s = df[column].astype(\"object\")   # breaks categorical\n",
        "    s = s.fillna(\"\")\n",
        "    s = s.astype(str)\n",
        "\n",
        "    return s.apply(\n",
        "        lambda x: category in [p.strip() for p in x.split(\",\") if p.strip()]\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "9nUpviCbRveZ"
      },
      "id": "9nUpviCbRveZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_irr_for_subgroup(df_agree, subgroup_mask, groups, n_levels=3):\n",
        "    subgroup_mask = subgroup_mask.reindex(df_agree.index).fillna(False)\n",
        "    n_raters = int(subgroup_mask.sum())\n",
        "\n",
        "    if n_raters < 2:\n",
        "        return pd.DataFrame([{\n",
        "            \"group\": \"ALL_ITEMS\",\n",
        "            \"num_raters\": n_raters,\n",
        "            \"num_items\": 0,\n",
        "            \"Fleiss_Kappa\": np.nan,\n",
        "            \"Krippendorff_Alpha\": np.nan\n",
        "        }])\n",
        "\n",
        "    sub_df = df_agree.loc[subgroup_mask]\n",
        "    results = []\n",
        "\n",
        "    # global\n",
        "    kappa_global = fleiss_kappa_from_matrix(sub_df)\n",
        "    alpha_global = krippendorff_alpha_ordinal(sub_df, n_levels=n_levels)\n",
        "    results.append({\n",
        "        \"group\": \"ALL_ITEMS\",\n",
        "        \"num_raters\": n_raters,\n",
        "        \"num_items\": sub_df.shape[1],\n",
        "        \"Fleiss_Kappa\": round(kappa_global, 3) if not np.isnan(kappa_global) else np.nan,\n",
        "        \"Krippendorff_Alpha\": round(alpha_global, 3) if not np.isnan(alpha_global) else np.nan\n",
        "    })\n",
        "\n",
        "    # per FM group\n",
        "    for gname, cols in groups.items():\n",
        "        cols_in_df = [c for c in cols if c in sub_df.columns]\n",
        "        if not cols_in_df:\n",
        "            continue\n",
        "        mat = sub_df[cols_in_df]\n",
        "        kappa_g = fleiss_kappa_from_matrix(mat)\n",
        "        alpha_g = krippendorff_alpha_ordinal(mat, n_levels=n_levels)\n",
        "\n",
        "        results.append({\n",
        "            \"group\": gname,\n",
        "            \"num_raters\": n_raters,\n",
        "            \"num_items\": len(cols_in_df),\n",
        "            \"Fleiss_Kappa\": round(kappa_g, 3) if not np.isnan(kappa_g) else np.nan,\n",
        "            \"Krippendorff_Alpha\": round(alpha_g, 3) if not np.isnan(alpha_g) else np.nan\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n"
      ],
      "metadata": {
        "id": "borpeEsxRx6r"
      },
      "id": "borpeEsxRx6r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "subgroup_irr_results = {}  # para guardar todo, por si luego quieres exportar\n",
        "\n",
        "\n",
        "# 1) Subgrupo: solo personas familiarizadas con SPL/FM notation (boolean, no multivaluado)\n",
        "if \"SPL_familiar\" in df.columns:\n",
        "    print(\"######### Subgroup: SPL_familiar #########\")\n",
        "\n",
        "    mask_spl_fam_true = df[\"SPL_familiar\"] == True\n",
        "    mask_spl_fam_false = df[\"SPL_familiar\"] == False\n",
        "\n",
        "    print(\"=== IRR for SPL_familiar == True ===\")\n",
        "    irr_spl_true = compute_irr_for_subgroup(df_fm_agreement, mask_spl_fam_true, groups)\n",
        "    display(irr_spl_true)\n",
        "    subgroup_irr_results[(\"SPL_familiar\", \"True\")] = irr_spl_true\n",
        "\n",
        "    print(\"\\n=== IRR for SPL_familiar == False ===\")\n",
        "    irr_spl_false = compute_irr_for_subgroup(df_fm_agreement, mask_spl_fam_false, groups)\n",
        "    display(irr_spl_false)\n",
        "    subgroup_irr_results[(\"SPL_familiar\", \"False\")] = irr_spl_false\n",
        "\n",
        "\n",
        "# 2) Subgrupos por experiencia en QC y en SPL (categoría simple, no multivaluada)\n",
        "for var in [\"exp_q\", \"exp_spl\"]:\n",
        "    if var not in df.columns:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n######### Subgroup analysis for {var} (single-valued) #########\")\n",
        "    levels = df[var].dropna().unique()\n",
        "    print(\"Levels:\", levels)\n",
        "\n",
        "    for level in levels:\n",
        "        mask = df[var] == level\n",
        "        n = int(mask.sum())\n",
        "        if n < 3:\n",
        "            print(f\"\\n--- {var} = {level} (n={n}) -> skipped (too few raters) ---\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n--- {var} = {level} (n={n}) ---\")\n",
        "        irr_df = compute_irr_for_subgroup(df_fm_agreement, mask, groups)\n",
        "        display(irr_df)\n",
        "        subgroup_irr_results[(var, str(level))] = irr_df\n",
        "\n",
        "\n",
        "# 3) Subgrupos por fase, rol, áreas de QC, país\n",
        "#    Detectamos automáticamente si son multivaluadas (valores con comas) o simples.\n",
        "for var in [\"phase\", \"role\", \"qc_areas\", \"country\"]:\n",
        "    if var not in df.columns:\n",
        "        continue\n",
        "\n",
        "    print(\"\\n\" + \"#\" * 60)\n",
        "    print(f\"### Subgroup analysis for {var}\")\n",
        "    print(\"#\" * 60)\n",
        "\n",
        "    # Convertimos a texto plano para inspeccionar si hay comas\n",
        "    col_str = df[var].astype(\"object\").astype(str).dropna()\n",
        "    is_multivalued = col_str.str.contains(\",\").any()\n",
        "\n",
        "    if is_multivalued:\n",
        "        print(f\"> Variable {var} tratada como MULTI-VALUED (multi-selección; valores separados por comas).\")\n",
        "        # Obtenemos las categorías individuales\n",
        "        categories = extract_unique_multicategories(df, var)\n",
        "        print(\"Detected atomic categories:\", categories)\n",
        "\n",
        "        for cat in categories:\n",
        "            mask = mask_for_multicategory(df, var, cat)\n",
        "            n = int(mask.sum())\n",
        "            if n < 3:\n",
        "                print(f\"\\n--- {var} includes '{cat}' (n={n}) -> skipped (too few raters) ---\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n--- {var} includes '{cat}' (n={n}) ---\")\n",
        "            irr_df = compute_irr_for_subgroup(df_fm_agreement, mask, groups)\n",
        "            display(irr_df)\n",
        "            subgroup_irr_results[(var, f\"includes:{cat}\")] = irr_df\n",
        "\n",
        "    else:\n",
        "        print(f\"> Variable {var} tratada como SINGLE-VALUED (una categoría por respuesta).\")\n",
        "        levels = df[var].dropna().unique()\n",
        "        print(\"Levels:\", levels)\n",
        "\n",
        "        for level in levels:\n",
        "            mask = df[var] == level\n",
        "            n = int(mask.sum())\n",
        "            if n < 3:\n",
        "                print(f\"\\n--- {var} = {level} (n={n}) -> skipped (too few raters) ---\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n--- {var} = {level} (n={n}) ---\")\n",
        "            irr_df = compute_irr_for_subgroup(df_fm_agreement, mask, groups)\n",
        "            display(irr_df)\n",
        "            subgroup_irr_results[(var, str(level))] = irr_df\n"
      ],
      "metadata": {
        "id": "F9XuNdFPR1eQ"
      },
      "id": "F9XuNdFPR1eQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Bootstrap Stability Analysis (Item-Level Positivity and I-CVI)\n",
        "\n",
        "To assess the robustness of the survey results, we compute **bootstrap confidence intervals** for:\n",
        "- the **proportion of positive responses** (`%positive`, i.e., rating ≥ 4), and  \n",
        "- the **Item Content Validity Index (I-CVI)**.\n",
        "\n",
        "Bootstrap resampling does not assume normality and is particularly suitable for:\n",
        "- ordinal Likert-5 data,\n",
        "- small or moderate sample sizes (N = 26 in our study),\n",
        "- skewed distributions (e.g., many 4–5 responses),\n",
        "- exploratory expert evaluations.\n",
        "\n",
        "For each item, we:\n",
        "1. Resample the participants **with replacement** (B = 1,000 by default).  \n",
        "2. Recompute `%positive` and I-CVI for each bootstrap sample.  \n",
        "3. Extract the **95% percentile interval**.  \n",
        "\n",
        "This provides a stability profile that distinguishes:\n",
        "- **Stable items** (narrow CIs well above the acceptance threshold),\n",
        "- **Borderline items** (CI crosses the decision threshold),\n",
        "- **Unstable items** (wide CIs or CIs entirely below the threshold).\n",
        "\n",
        "The result is a statistically grounded basis for accepting, revising, or reconsidering individual features in the Feature Model.\n"
      ],
      "metadata": {
        "id": "62fCNfkwYret"
      },
      "id": "62fCNfkwYret"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def bootstrap_proportion(x, B=1000, positive_threshold=4, random_state=None):\n",
        "    \"\"\"\n",
        "    Compute bootstrap distribution of the proportion of 'positive' responses\n",
        "    (defined as ratings >= positive_threshold).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : 1D array-like\n",
        "        Original item responses (Likert-5, with possible NaN).\n",
        "    B : int\n",
        "        Number of bootstrap resamples.\n",
        "    positive_threshold : int\n",
        "        Threshold above which responses are considered positive.\n",
        "    random_state : int or None\n",
        "        For reproducibility.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    boot_prop : ndarray of shape (B,)\n",
        "        Bootstrap replicate proportions.\n",
        "    \"\"\"\n",
        "\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    x = pd.to_numeric(pd.Series(x), errors=\"coerce\")\n",
        "    x = x.dropna()\n",
        "\n",
        "    if len(x) == 0:\n",
        "        return np.array([])\n",
        "\n",
        "    n = len(x)\n",
        "    boot_prop = np.zeros(B)\n",
        "\n",
        "    for b in range(B):\n",
        "        sample = x.iloc[rng.integers(0, n, size=n)]\n",
        "        boot_prop[b] = (sample >= positive_threshold).mean()\n",
        "\n",
        "    return boot_prop\n",
        "\n",
        "\n",
        "def bootstrap_icvi(x, B=1000, positive_threshold=4, random_state=None):\n",
        "    \"\"\"\n",
        "    Compute bootstrap distribution of I-CVI (proportion of positive responses)\n",
        "    using the same definition (ratings >= positive_threshold).\n",
        "\n",
        "    Equivalent to bootstrap_proportion, but kept separately for clarity.\n",
        "    \"\"\"\n",
        "\n",
        "    return bootstrap_proportion(\n",
        "        x,\n",
        "        B=B,\n",
        "        positive_threshold=positive_threshold,\n",
        "        random_state=random_state\n",
        "    )\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uLZHFz2MYz2b"
      },
      "id": "uLZHFz2MYz2b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def compute_bootstrap_ci_for_items(df, items, B=1000, alpha=0.05, positive_threshold=4):\n",
        "    \"\"\"\n",
        "    Compute bootstrap confidence intervals for %positive and I-CVI\n",
        "    for a given list of Likert-5 items.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Dataset of item responses.\n",
        "    items : list of str\n",
        "        Column names corresponding to FM Likert-5 items.\n",
        "    B : int\n",
        "        Number of bootstrap resamples.\n",
        "    alpha : float\n",
        "        Significance level (0.05 -> 95% CI).\n",
        "    positive_threshold : int\n",
        "        Threshold for positivity (default: rating >= 4).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame with:\n",
        "        - item\n",
        "        - N_valid\n",
        "        - proportion (original)\n",
        "        - prop_CI_low, prop_CI_high\n",
        "        - icvi (original)\n",
        "        - icvi_CI_low, icvi_CI_high\n",
        "    \"\"\"\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for col in items:\n",
        "        if col not in df.columns:\n",
        "            continue\n",
        "\n",
        "        s = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "        s_valid = s[(s >= 1) & (s <= 5)]  # ignore DK/NA coded as 0 or invalids\n",
        "        n_valid = len(s_valid)\n",
        "\n",
        "        if n_valid == 0:\n",
        "            rows.append({\n",
        "                \"item\": col,\n",
        "                \"N_valid\": 0,\n",
        "                \"proportion\": np.nan,\n",
        "                \"prop_CI_low\": np.nan,\n",
        "                \"prop_CI_high\": np.nan,\n",
        "                \"icvi\": np.nan,\n",
        "                \"icvi_CI_low\": np.nan,\n",
        "                \"icvi_CI_high\": np.nan\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Original statistics\n",
        "        prop = (s_valid >= positive_threshold).mean()\n",
        "        icvi = prop  # same definition\n",
        "\n",
        "        # Bootstrap distributions\n",
        "        boot_props = bootstrap_proportion(s_valid, B=B, positive_threshold=positive_threshold)\n",
        "        if len(boot_props) == 0:\n",
        "            ci_low = ci_high = np.nan\n",
        "        else:\n",
        "            ci_low = np.percentile(boot_props, 100 * (alpha / 2))\n",
        "            ci_high = np.percentile(boot_props, 100 * (1 - alpha / 2))\n",
        "\n",
        "        rows.append({\n",
        "            \"item\": col,\n",
        "            \"N_valid\": n_valid,\n",
        "            \"proportion\": round(prop, 3),\n",
        "            \"prop_CI_low\": round(ci_low, 3),\n",
        "            \"prop_CI_high\": round(ci_high, 3),\n",
        "            \"icvi\": round(icvi, 3),\n",
        "            \"icvi_CI_low\": round(ci_low, 3),\n",
        "            \"icvi_CI_high\": round(ci_high, 3)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "F1S0hFOYY2vV"
      },
      "id": "F1S0hFOYY2vV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bootstrap_results = {}\n",
        "\n",
        "for group_name, cols in groups.items():\n",
        "    print(f\"\\n### Bootstrap for FM group: {group_name}\")\n",
        "\n",
        "    df_ci = compute_bootstrap_ci_for_items(\n",
        "        df=df,\n",
        "        items=cols,\n",
        "        B=1000,\n",
        "        alpha=0.05,\n",
        "        positive_threshold=4\n",
        "    )\n",
        "\n",
        "    display(df_ci)\n",
        "    bootstrap_results[group_name] = df_ci\n"
      ],
      "metadata": {
        "id": "l78pt2grY419"
      },
      "id": "l78pt2grY419",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forest plot of bootstrap confidence intervals with traffic-light classification\n",
        "\n",
        "In the following plots, each feature is represented by:\n",
        "- a **point** showing the estimated proportion of positive responses (rating ≥ 4),\n",
        "- a **horizontal line** showing the 95% bootstrap confidence interval,\n",
        "- and a **vertical dashed line** at the acceptance threshold (0.65).\n",
        "\n",
        "We classify features according to their bootstrap confidence interval (CI):\n",
        "\n",
        "- **Green – Accepted (strong evidence)**  \n",
        "  The lower bound of the CI is **≥ 0.65**.  \n",
        "  → Even in the most conservative scenario, at least 650% of experts would rate the feature as relevant (≥ 4).  \n",
        "\n",
        "- **Amber – Limited evidence / borderline**  \n",
        "  The CI **crosses or touches** the decision region between 0.55 and 0.65.  \n",
        "  → There is some support, but the evidence is not strong enough to firmly conclude that the true proportion is above 0.65.  \n",
        "  → These features are good candidates for *refinement or closer inspection*.\n",
        "\n",
        "- **Red – Questionable / needs revision**  \n",
        "  The **upper bound** of the CI is **< 0.55**.  \n",
        "  → Even in the most optimistic scenario, fewer than 55% of experts would rate the feature as relevant.  \n",
        "  → These features should be reconsidered, reformulated, or possibly removed from the final model.\n",
        "\n",
        "This traffic-light classification provides an intuitive visual summary of which features are **robustly supported**, which are **borderline**, and which are **clearly weak** according to the expert survey.\n",
        "\n"
      ],
      "metadata": {
        "id": "rpqhA2dGbT1T"
      },
      "id": "rpqhA2dGbT1T"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "def classify_item_by_ci(row, accept_threshold=0.65, weak_threshold=0.55):\n",
        "    \"\"\"\n",
        "    Classify a feature according to:\n",
        "      - its point estimate of the proportion of positive responses\n",
        "      - and its bootstrap CI.\n",
        "\n",
        "    Rules (more inclusive):\n",
        "        - 'accepted' (green):\n",
        "              proportion >= accept_threshold\n",
        "        - 'questionable' (red):\n",
        "              prop_CI_high < weak_threshold\n",
        "        - 'limited' (amber):\n",
        "              all remaining cases\n",
        "    \"\"\"\n",
        "    p = row[\"proportion\"]\n",
        "    ci_low = row[\"prop_CI_low\"]\n",
        "    ci_high = row[\"prop_CI_high\"]\n",
        "\n",
        "    if pd.isna(p) or pd.isna(ci_low) or pd.isna(ci_high):\n",
        "        return \"unknown\"\n",
        "\n",
        "    if p >= accept_threshold:\n",
        "        return \"accepted\"\n",
        "    elif ci_high < weak_threshold:\n",
        "        return \"questionable\"\n",
        "        # note: even con p algo alto pero CI muy bajo, sería roja\n",
        "    else:\n",
        "        return \"limited\"\n",
        "\n",
        "\n",
        "def plot_forest_ci(df_ci, title=None,\n",
        "                   accept_threshold=0.65,\n",
        "                   weak_threshold=0.55):\n",
        "    \"\"\"\n",
        "    Forest plot for item-level proportions with bootstrap CIs, using a\n",
        "    traffic-light color scheme:\n",
        "\n",
        "        - green  = accepted (point estimate >= accept_threshold)\n",
        "        - amber  = limited evidence / borderline\n",
        "        - red    = questionable (CI_high < weak_threshold)\n",
        "\n",
        "    Items are ordered purely by decreasing 'proportion', so that the\n",
        "    best-supported items appear at the top and the weakest ones at the\n",
        "    bottom.\n",
        "\n",
        "    The figure height is adapted to the number of items, and the plotting\n",
        "    area width is kept constant by fixing the left margin, so that label\n",
        "    length does not affect the data region.\n",
        "    \"\"\"\n",
        "\n",
        "    # Drop rows without proportion\n",
        "    df_plot = df_ci.dropna(subset=[\"proportion\"]).copy()\n",
        "    if df_plot.empty:\n",
        "        print(\"No data to plot.\")\n",
        "        return\n",
        "\n",
        "    # Classify each item\n",
        "    df_plot[\"status\"] = df_plot.apply(\n",
        "        classify_item_by_ci,\n",
        "        axis=1,\n",
        "        accept_threshold=accept_threshold,\n",
        "        weak_threshold=weak_threshold,\n",
        "    )\n",
        "\n",
        "    # Map status to colors (traffic light)\n",
        "    color_map = {\n",
        "        \"accepted\": \"green\",\n",
        "        \"limited\": \"orange\",      # amber\n",
        "        \"questionable\": \"darkred\",\n",
        "        \"unknown\": \"gray\",\n",
        "    }\n",
        "    df_plot[\"color\"] = df_plot[\"status\"].map(color_map).fillna(\"gray\")\n",
        "\n",
        "    # ORDER: best to worst by proportion\n",
        "    df_plot = df_plot.sort_values(\n",
        "        by=\"proportion\",\n",
        "        ascending=False\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    n_items = len(df_plot)\n",
        "\n",
        "    # Compact figure height: ~0.32 per item, min 2.2, max 8\n",
        "    height = max(2.2, min(0.32 * n_items, 8))\n",
        "    figsize = (9, height)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    # y positions (one per item)\n",
        "    y_positions = range(n_items)\n",
        "\n",
        "    # Extract data arrays\n",
        "    props = df_plot[\"proportion\"].values\n",
        "    ci_low = df_plot[\"prop_CI_low\"].values\n",
        "    ci_high = df_plot[\"prop_CI_high\"].values\n",
        "    labels = df_plot[\"item\"].values\n",
        "    colors = df_plot[\"color\"].values\n",
        "\n",
        "    # Draw horizontal CI lines and points\n",
        "    for y, p, lo, hi, c in zip(y_positions, props, ci_low, ci_high, colors):\n",
        "        ax.hlines(y, lo, hi, color=c, linewidth=1.5)\n",
        "        ax.plot(p, y, \"o\", color=c, markersize=4)\n",
        "\n",
        "    # Vertical reference line at acceptance threshold\n",
        "    ax.axvline(x=accept_threshold, linestyle=\"--\")\n",
        "\n",
        "    # y-axis labels\n",
        "    ax.set_yticks(list(y_positions))\n",
        "    ax.set_yticklabels(labels)\n",
        "\n",
        "    # Pon esta línea justo después:\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "    ax.set_xlabel(\"Proportion of positive responses (rating ≥ 4)\")\n",
        "    ax.set_ylabel(\"Item\")\n",
        "\n",
        "    if title is not None:\n",
        "        ax.set_title(title)\n",
        "\n",
        "    ax.set_xlim(0, 1)  # proportions in [0, 1]\n",
        "\n",
        "    # Legend (small, bottom-right)\n",
        "    legend_patches = [\n",
        "        Patch(color=\"green\", label=f\"Accepted (p ≥ {accept_threshold:.2f})\"),\n",
        "        Patch(color=\"orange\", label=f\"Limited evidence\"),\n",
        "        Patch(color=\"darkred\", label=f\"Questionable (CI high < {weak_threshold:.2f})\"),\n",
        "    ]\n",
        "    ax.legend(handles=legend_patches, loc=\"upper left\", fontsize=\"small\")\n",
        "\n",
        "\n",
        "    # Fix left margin so that the width of the plotting area is constant\n",
        "    # across figures, regardless of label length.\n",
        "    fig.subplots_adjust(left=0.40, right=0.98)\n",
        "\n",
        "    # ¡OJO! No usamos tight_layout aquí, para no sobreescribir los márgenes.\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "-zBjdt5UbVXb"
      },
      "id": "-zBjdt5UbVXb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for group_name, df_ci_group in bootstrap_results.items():\n",
        "    title = f\"{group_name} – Bootstrap 95% CI (traffic-light classification)\"\n",
        "    plot_forest_ci(\n",
        "        df_ci_group,\n",
        "        title=title,\n",
        "        accept_threshold=0.65,\n",
        "        weak_threshold=0.55,\n",
        "    )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rCcZp42Ebhl0"
      },
      "id": "rCcZp42Ebhl0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Overall summary table of item-level reliability and stability\n",
        "\n",
        "The following table summarises, for each feature and constraint:\n",
        "\n",
        "- basic response statistics: `N_total`, `N_valid`, `DKNA_%`, `mean`, `median`, `sd`, `%neg`, `%neutral`, `%pos`;\n",
        "- content validity: `I-CVI` (proportion of ratings ≥ 4), `%pos`, and qualitative `status` (accepted / limited / questionable);\n",
        "- group-level agreement: `Fleiss_Kappa` and `Krippendorff_Alpha` (reported in subtotal rows per FM group);\n",
        "- bootstrap stability: `icvi`, `icvi_CI_low`, and `icvi_CI_high` (95% bootstrap confidence interval for I-CVI).\n",
        "\n",
        "Subtotal rows aggregate each FM group, reporting the average I-CVI, the number of items in each traffic-light class (Green/Amber/Red), and the corresponding inter-rater reliability indices."
      ],
      "metadata": {
        "id": "r6_Muz7uqlHf"
      },
      "id": "r6_Muz7uqlHf"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def classify_item_by_ci(row, accept_threshold=0.65, weak_threshold=0.55):\n",
        "    \"\"\"\n",
        "    Classify a feature according to:\n",
        "      - its point estimate of the proportion of positive responses\n",
        "      - and its bootstrap CI.\n",
        "\n",
        "    Rules (more inclusive):\n",
        "        - 'accepted':\n",
        "              proportion >= accept_threshold\n",
        "        - 'questionable':\n",
        "              prop_CI_high < weak_threshold\n",
        "        - 'limited':\n",
        "              all remaining cases\n",
        "    \"\"\"\n",
        "    p = row.get(\"proportion\", np.nan)\n",
        "    ci_low = row.get(\"prop_CI_low\", np.nan)\n",
        "    ci_high = row.get(\"prop_CI_high\", np.nan)\n",
        "\n",
        "    if pd.isna(p) or pd.isna(ci_low) or pd.isna(ci_high):\n",
        "        return \"unknown\"\n",
        "\n",
        "    if p >= accept_threshold:\n",
        "        return \"accepted\"\n",
        "    elif ci_high < weak_threshold:\n",
        "        return \"questionable\"\n",
        "    else:\n",
        "        return \"limited\"\n",
        "\n",
        "\n",
        "def build_final_reliability_table(df, df_fm_agreement, groups, bootstrap_results,\n",
        "                                  accept_threshold=0.65, weak_threshold=0.55):\n",
        "    \"\"\"\n",
        "    Build a consolidated table including, for each item:\n",
        "\n",
        "      From IRR / descriptives:\n",
        "        - item, N_total, N_valid, DKNA_%, mean, median, sd, %neg, %neutral, %pos\n",
        "\n",
        "      From I-CVI:\n",
        "        - I-CVI, %pos (duplicated), status (accepted/limited/questionable)\n",
        "\n",
        "      From IRR (point 3, group-level):\n",
        "        - Fleiss_Kappa, Krippendorff_Alpha\n",
        "        (only filled in subtotal rows per group)\n",
        "\n",
        "      From bootstrap:\n",
        "        - icvi, icvi_CI_low, icvi_CI_high\n",
        "        (item-level, taken from bootstrap_results[group])\n",
        "\n",
        "    Subtotals are added as separate rows after each group, using the 'item'\n",
        "    column (e.g., 'Subtotal gen_funct') and aggregating over the items in that group.\n",
        "    \"\"\"\n",
        "\n",
        "    all_rows = []\n",
        "    n_total = len(df)\n",
        "\n",
        "    # Precompute group-level IRR (Fleiss' Kappa and Krippendorff's Alpha)\n",
        "    irr_per_group = {}\n",
        "    if df_fm_agreement is not None:\n",
        "        for group_name, cols in groups.items():\n",
        "            cols_in_df = [c for c in cols if c in df_fm_agreement.columns]\n",
        "            if not cols_in_df:\n",
        "                irr_per_group[group_name] = (np.nan, np.nan)\n",
        "                continue\n",
        "\n",
        "            submatrix = df_fm_agreement[cols_in_df]\n",
        "            kappa_g = fleiss_kappa_from_matrix(submatrix)\n",
        "            alpha_g = krippendorff_alpha_ordinal(submatrix, n_levels=3)\n",
        "            irr_per_group[group_name] = (kappa_g, alpha_g)\n",
        "    else:\n",
        "        for group_name in groups.keys():\n",
        "            irr_per_group[group_name] = (np.nan, np.nan)\n",
        "\n",
        "    # Build item rows and subtotals\n",
        "    for group_name, items in groups.items():\n",
        "        group_rows = []\n",
        "\n",
        "        # Bootstrap table for this group (if available)\n",
        "        boot_df = bootstrap_results.get(group_name, None)\n",
        "\n",
        "        for col in items:\n",
        "            if col not in df.columns:\n",
        "                continue\n",
        "\n",
        "            # Raw Likert responses (0,1,2,3,4,5,...)\n",
        "            s = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "            # Valid 1..5\n",
        "            valid_mask = s.between(1, 5)\n",
        "            s_valid = s[valid_mask]\n",
        "            n_valid = int(valid_mask.sum())\n",
        "\n",
        "            dkna_count = n_total - n_valid\n",
        "            dkna_pct = 100.0 * dkna_count / n_total if n_total > 0 else np.nan\n",
        "\n",
        "            if n_valid > 0:\n",
        "                mean_val = s_valid.mean()\n",
        "                median_val = s_valid.median()\n",
        "                sd_val = s_valid.std(ddof=1)\n",
        "                neg_pct = 100.0 * ((s_valid <= 2).sum() / n_valid)\n",
        "                neut_pct = 100.0 * ((s_valid == 3).sum() / n_valid)\n",
        "                pos_pct = 100.0 * ((s_valid >= 4).sum() / n_valid)\n",
        "                icvi_val = (s_valid >= 4).mean()\n",
        "            else:\n",
        "                mean_val = median_val = sd_val = np.nan\n",
        "                neg_pct = neut_pct = pos_pct = np.nan\n",
        "                icvi_val = np.nan\n",
        "\n",
        "            # Bootstrap info for this item\n",
        "            icvi_boot = icvi_ci_low = icvi_ci_high = np.nan\n",
        "            status = \"unknown\"\n",
        "\n",
        "            if boot_df is not None and \"item\" in boot_df.columns:\n",
        "                row_b = boot_df[boot_df[\"item\"] == col]\n",
        "                if not row_b.empty:\n",
        "                    r0 = row_b.iloc[0]\n",
        "                    icvi_boot = r0.get(\"icvi\", np.nan)\n",
        "                    icvi_ci_low = r0.get(\"icvi_CI_low\", np.nan)\n",
        "                    icvi_ci_high = r0.get(\"icvi_CI_high\", np.nan)\n",
        "                    # classification using bootstrap proportion + CI\n",
        "                    status = classify_item_by_ci(\n",
        "                        r0,\n",
        "                        accept_threshold=accept_threshold,\n",
        "                        weak_threshold=weak_threshold,\n",
        "                    )\n",
        "            # If we do not have bootstrap for this item, fall back to icvi_val\n",
        "            if status == \"unknown\" and not np.isnan(icvi_val):\n",
        "                # simple classification from point estimate\n",
        "                if icvi_val >= accept_threshold:\n",
        "                    status = \"accepted\"\n",
        "                elif icvi_val < weak_threshold:\n",
        "                    status = \"questionable\"\n",
        "                else:\n",
        "                    status = \"limited\"\n",
        "\n",
        "            row = {\n",
        "                \"item\": col,\n",
        "                \"N_total\": n_total,\n",
        "                \"N_valid\": n_valid,\n",
        "                \"DKNA_%\": round(dkna_pct, 1) if not np.isnan(dkna_pct) else np.nan,\n",
        "                \"mean\": round(mean_val, 3) if not np.isnan(mean_val) else np.nan,\n",
        "                \"median\": round(median_val, 3) if not np.isnan(median_val) else np.nan,\n",
        "                \"sd\": round(sd_val, 3) if not np.isnan(sd_val) else np.nan,\n",
        "                \"%neg\": round(neg_pct, 1) if not np.isnan(neg_pct) else np.nan,\n",
        "                \"%neutral\": round(neut_pct, 1) if not np.isnan(neut_pct) else np.nan,\n",
        "                \"%pos\": round(pos_pct, 1) if not np.isnan(pos_pct) else np.nan,\n",
        "\n",
        "                # I-CVI block\n",
        "                \"I-CVI\": round(icvi_val, 3) if not np.isnan(icvi_val) else np.nan,\n",
        "                \"status\": status,\n",
        "\n",
        "                # IRR (point 3) – left blank at item level, filled only in subtotal\n",
        "                \"Fleiss_Kappa\": np.nan,\n",
        "                \"Krippendorff_Alpha\": np.nan,\n",
        "\n",
        "                # Bootstrap I-CVI CI\n",
        "                \"icvi\": round(icvi_boot, 3) if not np.isnan(icvi_boot) else np.nan,\n",
        "                \"icvi_CI_low\": round(icvi_ci_low, 3) if not np.isnan(icvi_ci_low) else np.nan,\n",
        "                \"icvi_CI_high\": round(icvi_ci_high, 3) if not np.isnan(icvi_ci_high) else np.nan,\n",
        "            }\n",
        "\n",
        "            group_rows.append(row)\n",
        "\n",
        "        # Append item rows for this group\n",
        "        all_rows.extend(group_rows)\n",
        "\n",
        "        # Subtotal row for this group (using the already computed item-level rows)\n",
        "        if group_rows:\n",
        "            group_df = pd.DataFrame(group_rows)\n",
        "            n_items = len(group_df)\n",
        "            mean_icvi = group_df[\"I-CVI\"].mean()\n",
        "\n",
        "\n",
        "\n",
        "            # Group-level IRR from precomputed values\n",
        "            kappa_g, alpha_g = irr_per_group.get(group_name, (np.nan, np.nan))\n",
        "\n",
        "            subtotal_label = f\"SUBTOTAL {group_name}\"\n",
        "\n",
        "            subtotal_row = {\n",
        "                \"item\": subtotal_label,\n",
        "                \"N_total\": \"\",\n",
        "                \"N_valid\": \"\",\n",
        "                \"DKNA_%\": \"\",\n",
        "                \"mean\": \"\",\n",
        "                \"median\": \"\",\n",
        "                \"sd\": \"\",\n",
        "                \"%neg\": \"\",\n",
        "                \"%neutral\": \"\",\n",
        "                \"%pos\": \"\",\n",
        "                \"I-CVI\": round(mean_icvi, 3) if not np.isnan(mean_icvi) else \"\",\n",
        "                \"status\": \"\",\n",
        "                \"Fleiss_Kappa\": round(kappa_g, 3) if not (kappa_g is None or np.isnan(kappa_g)) else \"\",\n",
        "                \"Krippendorff_Alpha\": round(alpha_g, 3) if not (alpha_g is None or np.isnan(alpha_g)) else \"\",\n",
        "                \"icvi\": \"\",\n",
        "                \"icvi_CI_low\": \"\",\n",
        "                \"icvi_CI_high\": \"\",\n",
        "            }\n",
        "\n",
        "            all_rows.append(subtotal_row)\n",
        "\n",
        "    final_df = pd.DataFrame(all_rows)\n",
        "    return final_df\n",
        "\n",
        "\n",
        "# Build and display the final summary table\n",
        "final_reliability_table = build_final_reliability_table(\n",
        "    df=df,\n",
        "    df_fm_agreement=df_fm_agreement,\n",
        "    groups=groups,\n",
        "    bootstrap_results=bootstrap_results,\n",
        "    accept_threshold=0.65,\n",
        "    weak_threshold=0.55,\n",
        ")\n",
        "\n",
        "display(final_reliability_table)\n",
        "\n",
        "# Optional: save for the paper\n",
        "# final_reliability_table.to_csv(OUTPUT_DIR / \"final_reliability_table.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "7zbFrjzonl0g"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7zbFrjzonl0g"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**8.** Analysis of Feature Model Constraints\n",
        "\n",
        "* **Descriptive analysis and data cleaning**  \n",
        "All internal (`c_`) and cross-subtree (`ccc_`) constraints are identified and assigned to Feature Model subtrees based on their variable names. Responses are cleaned to remove out-of-range values and to identify missing or DK/NA entries. Internal constraints are rescaled from a 1–5 to a unified 1–3 ordinal scale, and descriptive indicators (%negative, %neutral, %positive) are computed per constraint. Aggregated descriptive summaries are then produced per subtree, jointly analysing `c_` and `ccc_` constraints.\n",
        "\n",
        "* **Content validity assessment (I-CVI)**  \n",
        "Content validity is evaluated using the Item-Level Content Validity Index (I-CVI), computed as the proportion of experts providing a positive evaluation (category 3) for each constraint. Relaxed thresholds are applied to classify constraints as accepted, requiring revision, or rejection candidates. In addition, a subtree-level index (S-CVI/Ave) is computed as the mean I-CVI across all constraints within each subtree, providing an aggregated measure of content validity.\n",
        "\n",
        "* **Inter-rater reliability analysis**  \n",
        "Agreement among experts is assessed at the subtree level. Fleiss’ Kappa is computed at the item level and averaged within each subtree, while Krippendorff’s Alpha (ordinal) is computed using all constraints of a subtree as units. These complementary indicators quantify the consistency and semantic coherence of expert judgments within each block of constraints.\n",
        "\n",
        "* **Bootstrap-based robustness analysis**  \n",
        "The stability of the I-CVI estimates is analysed through non-parametric bootstrap resampling. For each constraint, bootstrap confidence intervals are computed, allowing the identification of strongly supported, borderline, and weakly supported constraints under uncertainty. Results are summarised in tables and visualised using forest plots, supporting more robust decisions on constraint acceptance or revision.\n"
      ],
      "metadata": {
        "id": "X8gwvKstvZwn"
      },
      "id": "X8gwvKstvZwn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Initial cleaning and descriptive analysis of constraints by category (subtree)\n",
        "\n",
        "In this section, we perform a systematic cleaning and an initial descriptive analysis of all items that represent Feature Model constraints evaluated by experts. Constraints are of two types:\n",
        "\n",
        "- **c_***: internal constraints within a subtree, originally measured on a 1–5 Likert scale.\n",
        "- **ccc_***: cross-subtree constraints, measured on a 1–3 Likert scale.\n",
        "\n",
        "Each item is associated with a **category or subtree** of the feature model. This category is identified from the variable name, taking the label that appears:\n",
        "\n",
        "- immediately after `c_` or `ccc_`,  \n",
        "- and before the next underscore (`_`).\n",
        "\n",
        "are all grouped under the same category **algo**, combining in a single block both internal constraints and cross-subtree constraints associated with that subtree.\n",
        "\n",
        "The procedure is as follows:\n",
        "\n",
        "1. **Automatic item selection**, splitting:\n",
        "   - Columns starting with `c_` (internal constraints).\n",
        "   - Columns starting with `ccc_` (cross-subtree constraints).\n",
        "2. **Category (subtree) identification** for each item based on its name, and assignment of this label to each constraint.\n",
        "3. **Value cleaning**:\n",
        "   - Detection of out-of-range responses.\n",
        "   - Identification of missing / DK/NA values and computation of their percentages.\n",
        "4. **Rescaling of the 1–5 Likert scale to a 1–3 scale** for `c_` constraints, using:\n",
        "   - 1–2 → 1 (negative response)\n",
        "   - 3 → 2 (neutral response)\n",
        "   - 4–5 → 3 (positive response)\n",
        "5. **Descriptive statistics per item**, on a 1–3 scale:\n",
        "   - N_total, N_valid, DKNA_%\n",
        "   - Percentage distribution for categories 1, 2 and 3\n",
        "   - %neg, %neutral, %pos\n",
        "   - Mean, median and standard deviation\n",
        "6. **Aggregated descriptors by category (subtree)**, combining together `c_` and `ccc_` constraints that share the same category label. That is, all variables whose names share the same subtree tag are integrated into a single summary block.\n",
        "\n",
        "As a result, we obtain two main tables:\n",
        "\n",
        "- **desc_all**: combined descriptives for all constraints (type `c` and `ccc`), with their corresponding category/subtree.\n",
        "- **desc_blocks**: aggregated summary by category/subtree, combining both `c_` and `ccc_` constraints for each subtree.\n",
        "\n",
        "These tables will serve as the basis for the subsequent validity and agreement analyses (IRR, I-CVI and bootstrap)."
      ],
      "metadata": {
        "id": "SQy1hiAtLuOU"
      },
      "id": "SQy1hiAtLuOU"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# POINT 1 — DESCRIPTIVES (COMBINED c_ AND ccc_) IN 1–3 SCALE\n",
        "# ================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Identify items\n",
        "# -------------------------------------------------------------------\n",
        "c_cols   = [col for col in df.columns if col.startswith(\"c_\")]\n",
        "ccc_cols = [col for col in df.columns if col.startswith(\"ccc_\")]\n",
        "\n",
        "df_c   = df[c_cols].copy()\n",
        "df_ccc = df[ccc_cols].copy()\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Extract subtree/categoría from naming pattern\n",
        "# e.g., c_algorithm_01  → \"algorithm\"\n",
        "#       ccc_backend_03 → \"backend\"\n",
        "# -------------------------------------------------------------------\n",
        "def extract_subtree(colname: str) -> str:\n",
        "    parts = colname.split(\"_\")\n",
        "    if len(parts) > 2 and parts[0] in (\"c\", \"ccc\"):\n",
        "        return parts[1]\n",
        "    return \"\"\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Clean Likert-like series, keeping only declared valid values\n",
        "# -------------------------------------------------------------------\n",
        "def clean_likert(series, valid_vals):\n",
        "    s = pd.to_numeric(series, errors=\"coerce\")\n",
        "    N_total = len(s)\n",
        "    mask_valid = s.isin(valid_vals)\n",
        "    N_valid = mask_valid.sum()\n",
        "    dkna_pct = 100 * (N_total - N_valid) / N_total if N_total > 0 else np.nan\n",
        "    s_clean = s.where(mask_valid, np.nan)\n",
        "    return s_clean, {\n",
        "        \"N_total\": N_total,\n",
        "        \"N_valid\": int(N_valid),\n",
        "        \"DKNA_%\": dkna_pct\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Recoding 1–5 → 1–3 for c_ variables\n",
        "# 1–2 → 1 ; 3 → 2 ; 4–5 → 3\n",
        "# -------------------------------------------------------------------\n",
        "def likert5_to3(x):\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    try:\n",
        "        v = int(x)\n",
        "    except:\n",
        "        return np.nan\n",
        "    if v in (1, 2):\n",
        "        return 1\n",
        "    if v == 3:\n",
        "        return 2\n",
        "    if v in (4, 5):\n",
        "        return 3\n",
        "    return np.nan\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Compute descriptives for c_ variables (recoded)\n",
        "# -------------------------------------------------------------------\n",
        "records_c = []\n",
        "\n",
        "for col in c_cols:\n",
        "    subtree = extract_subtree(col)\n",
        "\n",
        "    s_raw, info = clean_likert(df_c[col], valid_vals=[1,2,3,4,5])\n",
        "    s3 = s_raw.map(likert5_to3)\n",
        "\n",
        "    neg_pct = 100 * (s3 == 1).sum() / info[\"N_valid\"] if info[\"N_valid\"] > 0 else np.nan\n",
        "    neu_pct = 100 * (s3 == 2).sum() / info[\"N_valid\"] if info[\"N_valid\"] > 0 else np.nan\n",
        "    pos_pct = 100 * (s3 == 3).sum() / info[\"N_valid\"] if info[\"N_valid\"] > 0 else np.nan\n",
        "\n",
        "    records_c.append({\n",
        "        \"item\": col,\n",
        "        \"subtree\": subtree,\n",
        "        \"type\": \"c\",\n",
        "        \"N_total\": info[\"N_total\"],\n",
        "        \"N_valid\": info[\"N_valid\"],\n",
        "        \"DKNA_%\": info[\"DKNA_%\"],\n",
        "        \"%neg\": neg_pct,\n",
        "        \"%neutral\": neu_pct,\n",
        "        \"%pos\": pos_pct\n",
        "    })\n",
        "\n",
        "desc_c = pd.DataFrame(records_c)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Compute descriptives for ccc_ variables (already 1–3)\n",
        "# -------------------------------------------------------------------\n",
        "records_ccc = []\n",
        "\n",
        "for col in ccc_cols:\n",
        "    subtree = extract_subtree(col)\n",
        "\n",
        "    s_raw, info = clean_likert(df_ccc[col], valid_vals=[1,2,3])\n",
        "\n",
        "    neg_pct = 100 * (s_raw == 1).sum() / info[\"N_valid\"] if info[\"N_valid\"] > 0 else np.nan\n",
        "    neu_pct = 100 * (s_raw == 2).sum() / info[\"N_valid\"] if info[\"N_valid\"] > 0 else np.nan\n",
        "    pos_pct = 100 * (s_raw == 3).sum() / info[\"N_valid\"] if info[\"N_valid\"] > 0 else np.nan\n",
        "\n",
        "    records_ccc.append({\n",
        "        \"item\": col,\n",
        "        \"subtree\": subtree,\n",
        "        \"type\": \"ccc\",\n",
        "        \"N_total\": info[\"N_total\"],\n",
        "        \"N_valid\": info[\"N_valid\"],\n",
        "        \"DKNA_%\": info[\"DKNA_%\"],\n",
        "        \"%neg\": neg_pct,\n",
        "        \"%neutral\": neu_pct,\n",
        "        \"%pos\": pos_pct\n",
        "    })\n",
        "\n",
        "desc_ccc = pd.DataFrame(records_ccc)\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Combine both sets\n",
        "# -------------------------------------------------------------------\n",
        "desc_all = pd.concat([desc_c, desc_ccc], ignore_index=True)\n",
        "\n",
        "# Sort by subtree, type, item\n",
        "desc_all = desc_all.sort_values([\"subtree\", \"type\", \"item\"]).reset_index(drop=True)\n",
        "\n",
        "print(\"=== Descriptive table (desc_all) for c_ and ccc_ variables ===\")\n",
        "display(desc_all)\n"
      ],
      "metadata": {
        "id": "jtOi7n7jj537"
      },
      "id": "jtOi7n7jj537",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 7. Fragment plots — one per subtree (COMPACT VERSION)\n",
        "# ===============================================================\n",
        "for subtree, df_sub in desc_all.groupby(\"subtree\"):\n",
        "    n_items = len(df_sub)\n",
        "    height = max(2.2, 0.22 * n_items)   # Compact height\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, height))\n",
        "\n",
        "    items = df_sub[\"item\"]\n",
        "    neg = df_sub[\"%neg\"]\n",
        "    neu = df_sub[\"%neutral\"]\n",
        "    pos = df_sub[\"%pos\"]\n",
        "\n",
        "    ax.barh(items, neg, label=\"Negative\", color=\"#d73027\")\n",
        "    ax.barh(items, neu, left=neg, label=\"Neutral\", color=\"#fee08b\")\n",
        "    ax.barh(items, pos, left=neg + neu, label=\"Positive\", color=\"#1a9850\")\n",
        "\n",
        "    ax.set_title(f\"Fragment plot for subtree: {subtree}\", fontsize=10)\n",
        "    ax.set_xlabel(\"Percentage\", fontsize=9)\n",
        "    ax.set_yticklabels(items, fontsize=8)\n",
        "\n",
        "    ax.legend(loc=\"lower right\", fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 8. Global fragment plot — All constraints (COMPACT VERSION)\n",
        "# ===============================================================\n",
        "df_global = desc_all.sort_values([\"subtree\", \"type\", \"item\"]).reset_index(drop=True)\n",
        "\n",
        "n_items = len(df_global)\n",
        "height = max(3.0, 0.22 * n_items)   # Compact height\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, height))\n",
        "\n",
        "items = df_global[\"item\"]\n",
        "neg = df_global[\"%neg\"]\n",
        "neu = df_global[\"%neutral\"]\n",
        "pos = df_global[\"%pos\"]\n",
        "\n",
        "ax.barh(items, neg, label=\"Negative\", color=\"#d73027\")\n",
        "ax.barh(items, neu, left=neg, label=\"Neutral\", color=\"#fee08b\")\n",
        "ax.barh(items, pos, left=neg + neu, label=\"Positive\", color=\"#1a9850\")\n",
        "\n",
        "ax.set_title(\"Global fragment plot — All constraints (c_ + ccc_)\", fontsize=11)\n",
        "ax.set_xlabel(\"Percentage\", fontsize=9)\n",
        "ax.set_yticklabels(items, fontsize=8)\n",
        "\n",
        "ax.legend(loc=\"lower right\", fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Wiz-BnPbLt6v"
      },
      "id": "Wiz-BnPbLt6v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Content Validity: I-CVI per constraint and per subtree\n",
        "\n",
        "This section evaluates the **content validity** of each constraint using the  \n",
        "**Item-Level Content Validity Index (I-CVI)** on the unified 1–3 ordinal scale:\n",
        "\n",
        "- 1 = negative evaluation  \n",
        "- 2 = neutral  \n",
        "- 3 = positive / acceptable\n",
        "\n",
        "For each item (constraint), I-CVI is computed as:\n",
        "\n",
        "\\[\n",
        "\\text{I-CVI} = \\frac{\\#\\text{experts with rating } = 3}{N_\\text{valid}}\n",
        "\\]\n",
        "\n",
        "where \\(N_\\text{valid}\\) is the number of non-missing ratings for that item.\n",
        "\n",
        "The interpretation (with relaxed thresholds) is:\n",
        "\n",
        "- **I-CVI ≥ 0.65** → *Excellent* / acceptable content validity (item can be retained).  \n",
        "- **0.55 ≤ I-CVI < 0.65** → *Needs revision* (semantics should be clarified or reformulated).  \n",
        "- **I-CVI < 0.55** → *Rejection candidate* (insufficient support from experts).\n",
        "\n",
        "For each item we report:\n",
        "\n",
        "- `item`, `subtree`, `type` (c vs. ccc)  \n",
        "- `N_total`, `N_valid`, `DKNA_%`  \n",
        "- `%pos` (percentage of positive ratings, i.e. category 3)  \n",
        "- `I_CVI`  \n",
        "- `status` (categorical interpretation based on the thresholds above)\n",
        "\n",
        "In addition, we compute a **block-level Content Validity Index** per subtree (S-CVI/Ave):\n",
        "\n",
        "- **S-CVI/Ave** = mean I-CVI across all items in the subtree\n",
        "\n",
        "This provides an aggregated measure of how well each subtree is supported by expert judgments, combining both internal (`c_`) and cross-subtree (`ccc_`) constraints.\n",
        "\n",
        "The resulting tables are:\n",
        "\n",
        "- `icvi_table`: item-level content validity indices  \n",
        "- `icvi_blocks`: subtree-level S-CVI/Ave, plus a global S-CVI/Ave across all constraints\n"
      ],
      "metadata": {
        "id": "O3iPRMJAYGlU"
      },
      "id": "O3iPRMJAYGlU"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1. Item selection (same logic as in Point 1)\n",
        "# ----------------------------------------------------------\n",
        "c_cols   = [col for col in df.columns if col.startswith(\"c_\")]\n",
        "ccc_cols = [col for col in df.columns if col.startswith(\"ccc_\")]\n",
        "\n",
        "df_c   = df[c_cols].copy()\n",
        "df_ccc = df[ccc_cols].copy()\n",
        "\n",
        "def get_subtree_from_name(col_name: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract the subtree/category name from the column name.\n",
        "    Assumes pattern:\n",
        "      c_<SUBTREE>_...\n",
        "      ccc_<SUBTREE>_...\n",
        "    \"\"\"\n",
        "    parts = col_name.split(\"_\")\n",
        "    if len(parts) >= 2 and parts[0] in (\"c\", \"ccc\"):\n",
        "        return parts[1]\n",
        "    return parts[0] if parts else col_name\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2. Basic cleaning and rescaling (for c_ items)\n",
        "# ----------------------------------------------------------\n",
        "def clean_likert(series, valid_vals):\n",
        "    \"\"\"\n",
        "    Clean a Likert-type series:\n",
        "    - Cast to numeric\n",
        "    - Mark values outside valid_vals as NaN\n",
        "    Returns cleaned series and basic info dict.\n",
        "    \"\"\"\n",
        "    s = pd.to_numeric(series, errors=\"coerce\")\n",
        "    N_total = len(s)\n",
        "    mask_valid = s.isin(valid_vals)\n",
        "    N_valid = mask_valid.sum()\n",
        "    dkna_pct = 100 * (N_total - N_valid) / N_total if N_total > 0 else np.nan\n",
        "    s_clean = s.where(mask_valid, np.nan)\n",
        "    return s_clean, {\n",
        "        \"N_total\": N_total,\n",
        "        \"N_valid\": int(N_valid),\n",
        "        \"DKNA_%\": dkna_pct\n",
        "    }\n",
        "\n",
        "def likert5_to3(x):\n",
        "    \"\"\"\n",
        "    Map 1–5 Likert scale into 1–3:\n",
        "      1–2 -> 1 (negative)\n",
        "      3   -> 2 (neutral)\n",
        "      4–5 -> 3 (positive)\n",
        "    \"\"\"\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    try:\n",
        "        x = int(x)\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "    if x in (1, 2):\n",
        "        return 1\n",
        "    if x == 3:\n",
        "        return 2\n",
        "    if x in (4, 5):\n",
        "        return 3\n",
        "    return np.nan\n",
        "\n",
        "def classify_icvi(icvi, th1=0.55, th2=0.65):\n",
        "    \"\"\"\n",
        "    Classify I-CVI values into status categories using relaxed thresholds:\n",
        "      I-CVI >= th2      -> excellent\n",
        "      th1 <= I-CVI < th2 -> needs_revision\n",
        "      I-CVI < th1       -> reject\n",
        "    \"\"\"\n",
        "    if pd.isna(icvi):\n",
        "        return \"insufficient_data\"\n",
        "    if icvi >= th2:\n",
        "        return \"excellent\"\n",
        "    if icvi >= th1:\n",
        "        return \"needs_revision\"\n",
        "    return \"reject\"\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3. Compute I-CVI for c_ items (originally 1–5, rescaled to 1–3)\n",
        "# ----------------------------------------------------------\n",
        "records = []\n",
        "\n",
        "for col in c_cols:\n",
        "    subtree = get_subtree_from_name(col)\n",
        "    s_clean_5, info_5 = clean_likert(df_c[col], valid_vals=[1, 2, 3, 4, 5])\n",
        "    s_3 = s_clean_5.map(likert5_to3)  # 1–3 scale\n",
        "\n",
        "    N_valid = s_3.notna().sum()\n",
        "    if N_valid > 0:\n",
        "        n_pos = (s_3 == 3).sum()\n",
        "        icvi = n_pos / N_valid\n",
        "        pos_pct = 100 * n_pos / N_valid\n",
        "    else:\n",
        "        icvi = np.nan\n",
        "        pos_pct = np.nan\n",
        "\n",
        "    status = classify_icvi(icvi)\n",
        "\n",
        "    records.append({\n",
        "        \"item\": col,\n",
        "        \"subtree\": subtree,\n",
        "        \"type\": \"c\",\n",
        "        \"N_total\": info_5[\"N_total\"],\n",
        "        \"N_valid\": int(N_valid),\n",
        "        \"DKNA_%\": info_5[\"DKNA_%\"],\n",
        "        \"%pos\": pos_pct,\n",
        "        \"I_CVI\": icvi,\n",
        "        \"status\": status\n",
        "    })\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 4. Compute I-CVI for ccc_ items (already 1–3)\n",
        "# ----------------------------------------------------------\n",
        "for col in ccc_cols:\n",
        "    subtree = get_subtree_from_name(col)\n",
        "    s_clean_3, info_3 = clean_likert(df_ccc[col], valid_vals=[1, 2, 3])\n",
        "\n",
        "    N_valid = s_clean_3.notna().sum()\n",
        "    if N_valid > 0:\n",
        "        n_pos = (s_clean_3 == 3).sum()\n",
        "        icvi = n_pos / N_valid\n",
        "        pos_pct = 100 * n_pos / N_valid\n",
        "    else:\n",
        "        icvi = np.nan\n",
        "        pos_pct = np.nan\n",
        "\n",
        "    status = classify_icvi(icvi)\n",
        "\n",
        "    records.append({\n",
        "        \"item\": col,\n",
        "        \"subtree\": subtree,\n",
        "        \"type\": \"ccc\",\n",
        "        \"N_total\": info_3[\"N_total\"],\n",
        "        \"N_valid\": int(N_valid),\n",
        "        \"DKNA_%\": info_3[\"DKNA_%\"],\n",
        "        \"%pos\": pos_pct,\n",
        "        \"I_CVI\": icvi,\n",
        "        \"status\": status\n",
        "    })\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 5. Build I-CVI table (per item) and block-level summary\n",
        "# ----------------------------------------------------------\n",
        "icvi_table = pd.DataFrame(records)\n",
        "icvi_table = icvi_table.sort_values([\"subtree\", \"type\", \"item\"]).reset_index(drop=True)\n",
        "\n",
        "print(\"=== I-CVI per item (c_ and ccc_) ===\")\n",
        "display(icvi_table)\n",
        "\n",
        "# S-CVI/Ave per subtree\n",
        "icvi_blocks = (\n",
        "    icvi_table\n",
        "    .groupby(\"subtree\")[\"I_CVI\"]\n",
        "    .mean(numeric_only=True)\n",
        "    .reset_index()\n",
        "    .rename(columns={\"I_CVI\": \"S_CVI_Ave\"})\n",
        "    .sort_values(\"subtree\")\n",
        ")\n",
        "\n",
        "# Global S-CVI/Ave (all items)\n",
        "global_scvi_ave = icvi_table[\"I_CVI\"].mean(skipna=True)\n",
        "\n",
        "print(\"=== S-CVI/Ave by subtree ===\")\n",
        "display(icvi_blocks)\n",
        "\n",
        "print(f\"=== Global S-CVI/Ave across all constraints: {global_scvi_ave:.3f} ===\")\n"
      ],
      "metadata": {
        "id": "GNGY0Rw0YI3a"
      },
      "id": "GNGY0Rw0YI3a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# VISUALIZATIONS FOR I-CVI — COMPACT BARPLOT + COMPACT HEATMAP\n",
        "# ================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df_plot = icvi_table.sort_values(\n",
        "    [\"subtree\", \"type\", \"item\"],\n",
        "    ascending=[True, True, True]\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1. Horizontal barplot of I-CVI per item (COMPACT)\n",
        "# ----------------------------------------------------------\n",
        "n_items = len(df_plot)\n",
        "height = max(3.0, 0.22 * n_items)   # compact height\n",
        "\n",
        "plt.figure(figsize=(9, height))\n",
        "\n",
        "plt.barh(df_plot[\"item\"], df_plot[\"I_CVI\"], color=\"#1f78b4\")\n",
        "\n",
        "# Threshold lines\n",
        "plt.axvline(0.65, color=\"green\", linestyle=\"--\", label=\"Excellent (0.65)\")\n",
        "plt.axvline(0.55, color=\"orange\", linestyle=\"--\", label=\"Needs revision (0.55)\")\n",
        "\n",
        "plt.xlabel(\"I-CVI\", fontsize=9)\n",
        "plt.title(\"Item-Level Content Validity Index (I-CVI) per Constraint\", fontsize=10)\n",
        "plt.yticks(fontsize=8)\n",
        "plt.legend(fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2. Heatmap of I-CVI by item and subtree (COMPACT)\n",
        "# ----------------------------------------------------------\n",
        "heatmap_df = icvi_table.pivot(index=\"item\", columns=\"subtree\", values=\"I_CVI\")\n",
        "\n",
        "n_items = len(heatmap_df)\n",
        "height = max(3.0, 0.28 * n_items)   # compact height\n",
        "\n",
        "plt.figure(figsize=(9, height))\n",
        "\n",
        "sns.heatmap(\n",
        "    heatmap_df,\n",
        "    annot=True,\n",
        "    cmap=\"viridis\",\n",
        "    fmt=\".2f\",\n",
        "    cbar_kws={\"label\": \"I-CVI\"},\n",
        "    linewidths=0.5,\n",
        "    annot_kws={\"fontsize\": 7}\n",
        ")\n",
        "\n",
        "plt.title(\"I-CVI Heatmap by Item and Subtree\", fontsize=10)\n",
        "plt.xlabel(\"Subtree\", fontsize=9)\n",
        "plt.ylabel(\"Item\", fontsize=9)\n",
        "plt.yticks(fontsize=7)\n",
        "plt.xticks(fontsize=8, rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "QmACGrhIZEDN"
      },
      "id": "QmACGrhIZEDN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Inter-Rater Reliability (IRR) by Category/Subtree\n",
        "\n",
        "This section evaluates the agreement among experts for each of the five major categories (subtrees) of the Feature Model.  \n",
        "Unlike Fleiss’ Kappa, which is well-defined at the item level, Krippendorff’s Alpha (ordinal) cannot be computed for a single item because its denominator requires variability across multiple units. Therefore:\n",
        "\n",
        "- **Fleiss’ Kappa** is computed at the *item level* and then averaged within each subtree.\n",
        "- **Krippendorff’s Alpha (ordinal)** is computed at the *subtree level*, using all items of that subtree as “units”, which is the correct statistical formulation.\n",
        "\n",
        "## Indicators computed per subtree\n",
        "For each of the five subtrees, two indicators are reported:\n",
        "\n",
        "1. **Fleiss’ Kappa (average across items of the subtree)**  \n",
        "   - Measures categorical agreement between experts.  \n",
        "   - Sensitive to marginal distributions (e.g., strong skew in responses reduces Kappa).\n",
        "\n",
        "2. **Krippendorff’s Alpha (ordinal)**  \n",
        "   - Measures reliability of ordinal ratings (1–3).  \n",
        "   - Handles missing values naturally.  \n",
        "   - Uses ordinal distances, therefore capturing disagreement severity.\n",
        "\n",
        "Together, these indicators provide a robust view of the strength and consistency of expert judgments for each block of constraints.\n",
        "\n",
        "## Interpretation guidelines\n",
        "\n",
        "### **Fleiss’ Kappa (κ)**\n",
        "General interpretation (Landis & Koch, 1977):\n",
        "- **κ < 0.00** → Poor agreement  \n",
        "- **0.00–0.20** → Slight  \n",
        "- **0.21–0.40** → Fair  \n",
        "- **0.41–0.60** → Moderate  \n",
        "- **0.61–0.80** → Substantial  \n",
        "- **0.81–1.00** → Almost perfect  \n",
        "\n",
        "Since constraints often have skewed category usage, Kappa tends to be **conservative**.\n",
        "\n",
        "### **Krippendorff’s Alpha (ordinal)**\n",
        "Standard interpretation:\n",
        "- **α ≥ 0.80** → Strong reliability  \n",
        "- **0.67 ≤ α < 0.80** → Tentative / acceptable  \n",
        "- **α < 0.67** → Insufficient reliability  \n",
        "\n",
        "Alpha is more robust than Kappa and is not biased by skewed marginal distributions, so it provides a clearer picture of semantic coherence within each subtree.\n",
        "\n",
        "## Heatmap\n",
        "A heatmap summarises, for each subtree:\n",
        "\n",
        "- The average Fleiss’ Kappa  \n",
        "- Krippendorff’s Alpha (ordinal)\n",
        "\n",
        "This visualization helps identify:\n",
        "- **Highly reliable blocks** (high κ and α)  \n",
        "- **Blocks with moderate consensus** (mid-range values)  \n",
        "- **Problematic or ambiguous blocks** (low κ and α)  \n",
        "\n",
        "These indicators guide revision decisions in the next sections (content validity, I-CVI, etc.).\n"
      ],
      "metadata": {
        "id": "QVojvixwUoso"
      },
      "id": "QVojvixwUoso"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# IRR BY SUBTREE: Fleiss' Kappa (mean per block) + Alpha (ordinal)\n",
        "# ================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "    import krippendorff\n",
        "except ModuleNotFoundError:\n",
        "    !pip install krippendorff\n",
        "    import krippendorff\n",
        "from statsmodels.stats.inter_rater import fleiss_kappa\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1. Fleiss' Kappa per item (already available in irr_table)\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "# If irr_table was not created yet, regenerate minimally:\n",
        "def compute_fleiss_kappa_single(series):\n",
        "    s = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
        "    if len(s) == 0:\n",
        "        return np.nan\n",
        "    cats = [1, 2, 3]\n",
        "    table = np.zeros((len(s), len(cats)))\n",
        "    for i, val in enumerate(s):\n",
        "        if val in cats:\n",
        "            table[i, cats.index(val)] = 1\n",
        "    # Convert to category-counts matrix\n",
        "    count_matrix = np.array([table[:, j].sum() for j in range(len(cats))])\n",
        "    mat = np.array([count_matrix])\n",
        "    try:\n",
        "        return float(fleiss_kappa(mat))\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "records = []\n",
        "for item in desc_all[\"item\"]:\n",
        "    s = df[item]\n",
        "    s_clean = pd.to_numeric(s, errors=\"coerce\")\n",
        "    kappa = compute_fleiss_kappa_single(s_clean)\n",
        "    records.append({\n",
        "        \"item\": item,\n",
        "        \"subtree\": desc_all.set_index(\"item\").loc[item, \"subtree\"],\n",
        "        \"type\": desc_all.set_index(\"item\").loc[item, \"type\"],\n",
        "        \"Fleiss_Kappa\": kappa\n",
        "    })\n",
        "\n",
        "irr_items = pd.DataFrame(records)\n",
        "\n",
        "# Average Fleiss' Kappa per subtree\n",
        "kappa_blocks = (\n",
        "    irr_items.groupby(\"subtree\")[\"Fleiss_Kappa\"]\n",
        "    .mean(numeric_only=True)\n",
        "    .reset_index()\n",
        "    .sort_values(\"subtree\")\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2. Krippendorff’s Alpha (ordinal) per subtree\n",
        "# ----------------------------------------------------------\n",
        "def compute_alpha_matrix(df_matrix):\n",
        "    data = df_matrix.to_numpy()\n",
        "    try:\n",
        "        return float(krippendorff.alpha(\n",
        "            reliability_data=data,\n",
        "            level_of_measurement='ordinal'\n",
        "        ))\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "alpha_records = []\n",
        "\n",
        "df_items_numeric = df[desc_all[\"item\"]].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "for subtree, group in desc_all.groupby(\"subtree\"):\n",
        "    cols = group[\"item\"].tolist()\n",
        "    df_sub = df_items_numeric[cols]\n",
        "    alpha_sub = compute_alpha_matrix(df_sub.T)\n",
        "    alpha_records.append({\"subtree\": subtree, \"Krippendorff_Alpha\": alpha_sub})\n",
        "\n",
        "alpha_blocks = pd.DataFrame(alpha_records).sort_values(\"subtree\").reset_index(drop=True)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3. Combine both indicators\n",
        "# ----------------------------------------------------------\n",
        "irr_blocks = pd.merge(kappa_blocks, alpha_blocks, on=\"subtree\")\n",
        "\n",
        "print(\"=== IRR by subtree (Fleiss' Kappa + Krippendorff's Alpha) ===\")\n",
        "display(irr_blocks)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 4. Heatmap of IRR indicators by subtree\n",
        "# ----------------------------------------------------------\n",
        "fig, ax = plt.subplots(figsize=(6, 0.6 * len(irr_blocks)))\n",
        "\n",
        "heatmap_data = irr_blocks.set_index(\"subtree\")[[\"Fleiss_Kappa\", \"Krippendorff_Alpha\"]]\n",
        "\n",
        "im = ax.imshow(heatmap_data, aspect='auto', cmap=\"viridis\", vmin=-1, vmax=1)\n",
        "\n",
        "ax.set_xticks([0, 1])\n",
        "ax.set_xticklabels([\"Fleiss' Kappa\", \"Krippendorff's Alpha\"], rotation=45, ha=\"right\")\n",
        "ax.set_yticks(range(len(heatmap_data)))\n",
        "ax.set_yticklabels(heatmap_data.index)\n",
        "\n",
        "ax.set_title(\"IRR Heatmap by Subtree\")\n",
        "plt.colorbar(im, ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DUaqYCHtUudZ"
      },
      "id": "DUaqYCHtUudZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Bootstrap Confidence Intervals for I-CVI\n",
        "\n",
        "This section estimates the **stability** of the Item-Level Content Validity Index (I-CVI) using **non-parametric bootstrap resampling**.\n",
        "\n",
        "For each constraint (item), we repeatedly resample expert ratings with replacement and recompute I-CVI. This produces a bootstrap distribution from which we extract:\n",
        "\n",
        "- **I-CVI_boot**: bootstrap mean  \n",
        "- **I-CVI_CI_low**: 2.5th percentile  \n",
        "- **I-CVI_CI_high**: 97.5th percentile  \n",
        "\n",
        "These intervals provide information about:\n",
        "\n",
        "### 1. Stability of the consensus\n",
        "Narrow confidence intervals indicate consistent judgments; wide intervals suggest heterogeneity or uncertainty among experts.\n",
        "\n",
        "### 2. Robustness with respect to the relaxed thresholds (0.55 and 0.65)\n",
        "\n",
        "- **CI entirely ≥ 0.65** → the item shows strong and stable content validity.  \n",
        "- **CI entirely < 0.55** → the item is not supported even under uncertainty; revision or removal is recommended.  \n",
        "- **CI overlapping the [0.55, 0.65] region** → the item is borderline: it may be acceptable but fragile and should be examined in detail.\n",
        "\n",
        "### 3. Sensitivity to sample size\n",
        "Bootstrap compensates for the relatively small number of raters typical in expert validation studies, providing more reliable inferences about I-CVI.\n",
        "\n",
        "The final output includes:\n",
        "\n",
        "- `icvi_bootstrap_table`: item-level bootstrap estimates and confidence intervals  \n",
        "- A combined table joining raw I-CVI, bootstrap results and classification  \n",
        "- A “forest plot” visualisation with point estimates and confidence intervals for all items\n"
      ],
      "metadata": {
        "id": "doZEwV9IbwL-"
      },
      "id": "doZEwV9IbwL-"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# UPDATED CLASSIFICATION (SEPARATED COLUMNS + NEW THRESHOLDS)\n",
        "# Thresholds: 0.55 (lower), 0.65 (upper)\n",
        "# Two status columns: status_base, status_ci\n",
        "# ================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# 0. Ensure icvi_bootstrap_table exists; if not, compute it here\n",
        "# ----------------------------------------------------------------\n",
        "if \"icvi_bootstrap_table\" not in globals():\n",
        "    print(\"icvi_bootstrap_table not found — computing bootstrap table now...\")\n",
        "\n",
        "    B = 10000  # number of bootstrap iterations\n",
        "    bootstrap_records = []\n",
        "\n",
        "    for _, row in icvi_table.iterrows():\n",
        "        item = row[\"item\"]\n",
        "        subtree = row[\"subtree\"]\n",
        "        type_ = row[\"type\"]\n",
        "\n",
        "        # Extract ratings for this item\n",
        "        s = pd.to_numeric(df[item], errors=\"coerce\").dropna()\n",
        "        if s.empty:\n",
        "            bootstrap_records.append({\n",
        "                \"item\": item,\n",
        "                \"subtree\": subtree,\n",
        "                \"type\": type_,\n",
        "                \"I_CVI_boot\": np.nan,\n",
        "                \"I_CVI_CI_low\": np.nan,\n",
        "                \"I_CVI_CI_high\": np.nan\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        positive = (s == 3).astype(int)\n",
        "        N = len(positive)\n",
        "\n",
        "        boot_icvi = []\n",
        "        for _ in range(B):\n",
        "            sample = positive.sample(N, replace=True)\n",
        "            boot_icvi.append(sample.mean())\n",
        "\n",
        "        boot_icvi = np.array(boot_icvi)\n",
        "\n",
        "        bootstrap_records.append({\n",
        "            \"item\": item,\n",
        "            \"subtree\": subtree,\n",
        "            \"type\": type_,\n",
        "            \"I_CVI_boot\": boot_icvi.mean(),\n",
        "            \"I_CVI_CI_low\": np.percentile(boot_icvi, 2.5),\n",
        "            \"I_CVI_CI_high\": np.percentile(boot_icvi, 97.5)\n",
        "        })\n",
        "\n",
        "    icvi_bootstrap_table = pd.DataFrame(bootstrap_records)\n",
        "    print(\"Bootstrap table 'icvi_bootstrap_table' created.\")\n",
        "    display(icvi_bootstrap_table.head())\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# 1. Merge raw I-CVI with bootstrap results\n",
        "# ----------------------------------------------------------------\n",
        "icvi_full = pd.merge(\n",
        "    icvi_table,\n",
        "    icvi_bootstrap_table,\n",
        "    on=[\"item\", \"subtree\", \"type\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# 2. Base status from I-CVI (Point 3) with 0.55 / 0.65\n",
        "# ----------------------------------------------------------------\n",
        "def classify_base(icvi, th1=0.55, th2=0.65):\n",
        "    if pd.isna(icvi):\n",
        "        return \"insufficient_data\"\n",
        "    if icvi >= th2:\n",
        "        return \"excellent\"\n",
        "    if icvi >= th1:\n",
        "        return \"needs_revision\"\n",
        "    return \"reject\"\n",
        "\n",
        "icvi_full[\"status_base\"] = icvi_full[\"I_CVI\"].apply(classify_base)\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# 3. CI-based status (Point 4) with 0.55 / 0.65\n",
        "# ----------------------------------------------------------------\n",
        "def classify_by_ci(row, th1=0.55, th2=0.65):\n",
        "    low = row[\"I_CVI_CI_low\"]\n",
        "    high = row[\"I_CVI_CI_high\"]\n",
        "\n",
        "    if pd.isna(low) or pd.isna(high):\n",
        "        return \"insufficient_data\"\n",
        "\n",
        "    if low >= th2:\n",
        "        return \"robust_valid\"\n",
        "    if high < th1:\n",
        "        return \"not_supported\"\n",
        "    return \"borderline\"\n",
        "\n",
        "icvi_full[\"status_ci\"] = icvi_full.apply(classify_by_ci, axis=1)\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# 4. Optional final combined label\n",
        "# ----------------------------------------------------------------\n",
        "def combine(row):\n",
        "    return f\"{row['status_base']} | {row['status_ci']}\"\n",
        "\n",
        "icvi_full[\"status_final\"] = icvi_full.apply(combine, axis=1)\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# 5. Sorting and display\n",
        "# ----------------------------------------------------------------\n",
        "icvi_full = icvi_full.sort_values(\n",
        "    [\"subtree\", \"type\", \"item\"]\n",
        ").reset_index(drop=True)\n",
        "\n",
        "print(\"=== Combined I-CVI + Bootstrap + Updated Status (55/65 thresholds) ===\")\n",
        "display(icvi_full)\n"
      ],
      "metadata": {
        "id": "D-NZU01tbyHF"
      },
      "id": "D-NZU01tbyHF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# COMPACT FOREST PLOT WITH TRAFFIC-LIGHT COLORS AND SCIENTIFIC LEGEND\n",
        "# Thresholds: 0.55 (weak) / 0.65 (accept)\n",
        "# ================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "accept_threshold = 0.65\n",
        "weak_threshold   = 0.55\n",
        "\n",
        "plot_df = icvi_full.copy()\n",
        "\n",
        "# Fallback on I_CVI if bootstrap point is missing\n",
        "plot_df[\"I_CVI_point\"] = np.where(\n",
        "    plot_df[\"I_CVI_boot\"].notna(),\n",
        "    plot_df[\"I_CVI_boot\"],\n",
        "    plot_df[\"I_CVI\"]\n",
        ")\n",
        "\n",
        "plot_df = plot_df.sort_values([\"subtree\", \"type\", \"item\"]).reset_index(drop=True)\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# Assign colors to the POINT estimate only (traffic-light logic)\n",
        "# -----------------------------------------------------------------------\n",
        "def get_point_color(point):\n",
        "    if pd.isna(point):\n",
        "        return \"grey\"\n",
        "    if point >= accept_threshold:\n",
        "        return \"green\"\n",
        "    if point >= weak_threshold:\n",
        "        return \"orange\"\n",
        "    return \"darkred\"\n",
        "\n",
        "plot_df[\"color\"] = plot_df[\"I_CVI_point\"].apply(get_point_color)\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# Figure size: as compact as possible, but readable\n",
        "# Height proportional to number of items, with a small minimum\n",
        "# -----------------------------------------------------------------------\n",
        "n_items = len(plot_df)\n",
        "height = max(3.5, 0.25 * n_items)   # 0.25 per item, but not less than 3.5\n",
        "fig, ax = plt.subplots(figsize=(8, height))\n",
        "\n",
        "y_positions = np.arange(n_items)\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# Plot each item: point + CI\n",
        "# -----------------------------------------------------------------------\n",
        "for i, row in plot_df.iterrows():\n",
        "    point = row[\"I_CVI_point\"]\n",
        "    low   = row[\"I_CVI_CI_low\"]\n",
        "    high  = row[\"I_CVI_CI_high\"]\n",
        "    color = row[\"color\"]\n",
        "\n",
        "    if pd.isna(point) or pd.isna(low) or pd.isna(high):\n",
        "        continue\n",
        "\n",
        "    ax.errorbar(\n",
        "        x=point,\n",
        "        y=i,\n",
        "        xerr=[[point - low], [high - point]],\n",
        "        fmt=\"o\",\n",
        "        color=color,\n",
        "        ecolor=color,\n",
        "        capsize=3,\n",
        "        markersize=5,\n",
        "        linewidth=1\n",
        "    )\n",
        "\n",
        "# Threshold reference lines\n",
        "ax.axvline(accept_threshold, linestyle=\"--\", color=\"green\")\n",
        "ax.axvline(weak_threshold,   linestyle=\"--\", color=\"orange\")\n",
        "\n",
        "ax.set_yticks(y_positions)\n",
        "ax.set_yticklabels(plot_df[\"item\"], fontsize=8)\n",
        "ax.set_xlabel(\"I-CVI (bootstrap point estimate and 95% CI)\")\n",
        "ax.set_title(\"Forest plot of I-CVI with bootstrap CI (traffic-light coding)\", fontsize=10)\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# Legend with requested labels (upper left)\n",
        "# -----------------------------------------------------------------------\n",
        "legend_elements = [\n",
        "    Patch(color=\"green\",   label=f\"Accepted (p ≥ {accept_threshold:.2f})\"),\n",
        "    Patch(color=\"orange\",  label=\"Limited evidence\"),\n",
        "    Patch(color=\"darkred\", label=f\"Questionable (CI high < {weak_threshold:.2f})\"),\n",
        "]\n",
        "\n",
        "ax.legend(handles=legend_elements, loc=\"upper left\", fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PQR0_Atsg9OW"
      },
      "id": "PQR0_Atsg9OW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.** Categorical Variables"
      ],
      "metadata": {
        "id": "qqWYE4KcsZh6"
      },
      "id": "qqWYE4KcsZh6"
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency tables for categorical variables (no missing values assumed)\n",
        "# Robust multi-response counting:\n",
        "# - Unfolds multiple selections per respondent\n",
        "# - Counts ALL appearances across the dataset\n",
        "# - Forces every token to a string (prevents \"unhashable type\" errors)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from IPython.display import display\n",
        "\n",
        "SINGLE_CHOICE = [\"exp_q\", \"exp_spl\", \"country\"]\n",
        "MULTI_RESPONSE = [\"phase\", \"role\", \"qc_areas\"]\n",
        "\n",
        "def _iter_tokens(x, seps=(\";\", \",\", \"|\")):\n",
        "    \"\"\"\n",
        "    Yield clean string tokens from a cell that may contain:\n",
        "      - strings (possibly delimited)\n",
        "      - lists/tuples/sets (possibly nested)\n",
        "      - numpy arrays / pandas Series\n",
        "      - other scalars\n",
        "    This function is deliberately defensive and guarantees that yielded tokens are strings.\n",
        "    \"\"\"\n",
        "    # Treat missing as absent (you said there is no missing, but guard anyway)\n",
        "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
        "        return\n",
        "\n",
        "    # If it's a string, split on the first separator found; else keep as a single token\n",
        "    if isinstance(x, str):\n",
        "        s = x.strip()\n",
        "        if not s:\n",
        "            return\n",
        "        for sep in seps:\n",
        "            if sep in s:\n",
        "                for part in s.split(sep):\n",
        "                    t = part.strip()\n",
        "                    if t:\n",
        "                        yield t\n",
        "                return\n",
        "        yield s\n",
        "        return\n",
        "\n",
        "    # If it's an iterable container (including nested), iterate recursively\n",
        "    if isinstance(x, (list, tuple, set, np.ndarray, pd.Series)):\n",
        "        for item in x:\n",
        "            yield from _iter_tokens(item, seps=seps)\n",
        "        return\n",
        "\n",
        "    # Fallback: any other scalar -> single token\n",
        "    t = str(x).strip()\n",
        "    if t:\n",
        "        yield t\n",
        "\n",
        "def freq_single_choice(df, col):\n",
        "    \"\"\"\n",
        "    Frequency table for single-choice categorical variables.\n",
        "    Assumes no missing values.\n",
        "    \"\"\"\n",
        "    s = df[col].astype(str).str.strip()\n",
        "    n_total = len(s)\n",
        "\n",
        "    vc = s.value_counts(dropna=False)\n",
        "    out = pd.DataFrame({\n",
        "        \"variable\": col,\n",
        "        \"category\": vc.index.astype(str),\n",
        "        \"n\": vc.values,\n",
        "    })\n",
        "    out[\"pct\"] = out[\"n\"] / n_total * 100\n",
        "    out[\"n_total_rows\"] = n_total\n",
        "    return out\n",
        "\n",
        "def freq_multi_response_all_appearances(df, col, seps=(\";\", \",\", \"|\")):\n",
        "    \"\"\"\n",
        "    Frequency table for multi-response variables:\n",
        "    - Unfolds each cell into multiple selections (tokens)\n",
        "    - Counts ALL appearances across the dataset\n",
        "    Assumes no missing values.\n",
        "    \"\"\"\n",
        "    n_rows = len(df)\n",
        "\n",
        "    counter = Counter()\n",
        "    total_selections = 0\n",
        "\n",
        "    for x in df[col].tolist():\n",
        "        tokens = list(_iter_tokens(x, seps=seps))\n",
        "        counter.update(tokens)\n",
        "        total_selections += len(tokens)\n",
        "\n",
        "    if total_selections == 0:\n",
        "        # Edge case: column exists but no tokens were produced (e.g., empty strings everywhere)\n",
        "        return pd.DataFrame({\n",
        "            \"variable\": [col],\n",
        "            \"category\": [\"(no selections)\"],\n",
        "            \"n\": [0],\n",
        "            \"pct_rows\": [0.0],\n",
        "            \"pct_selections\": [0.0],\n",
        "            \"n_total_rows\": [n_rows],\n",
        "            \"n_total_selections\": [0]\n",
        "        })\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        \"variable\": col,\n",
        "        \"category\": list(counter.keys()),\n",
        "        \"n\": list(counter.values())\n",
        "    }).sort_values(\"n\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    out[\"pct_rows\"] = out[\"n\"] / n_rows * 100\n",
        "    out[\"pct_selections\"] = out[\"n\"] / total_selections * 100\n",
        "    out[\"n_total_rows\"] = n_rows\n",
        "    out[\"n_total_selections\"] = total_selections\n",
        "    return out\n",
        "\n",
        "# --- Run and display ---\n",
        "freq_tables = {}\n",
        "all_tables = []\n",
        "\n",
        "for col in SINGLE_CHOICE:\n",
        "    t = freq_single_choice(df, col)\n",
        "    freq_tables[col] = t\n",
        "    all_tables.append(t)\n",
        "    print(f\"\\n=== {col} (single-choice) ===\")\n",
        "    display(t[[\"category\", \"n\", \"pct\", \"n_total_rows\"]])\n",
        "\n",
        "for col in MULTI_RESPONSE:\n",
        "    t = freq_multi_response_all_appearances(df, col)\n",
        "    freq_tables[col] = t\n",
        "    all_tables.append(t)\n",
        "    print(f\"\\n=== {col} (multi-response; all appearances) ===\")\n",
        "    display(t[[\"category\", \"n\", \"pct_rows\", \"pct_selections\", \"n_total_rows\", \"n_total_selections\"]])\n",
        "\n",
        "freq_summary = pd.concat(all_tables, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "85ABjJGksXuV"
      },
      "id": "85ABjJGksXuV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Horizontal bar plots only (compact, consistent aesthetics, x-axis always 0–100)\n",
        "# Implemented requirements:\n",
        "# - exp_q: order existing dataset categories logically: beginner -> intermediate -> advanced -> expert\n",
        "#   (no invented categories; no Spanish variants; unknown/other categories appended after the known ones)\n",
        "# - phase: ordered using dataset categories matching: analysis -> design -> coding -> testing -> execution\n",
        "#   (unknown/other categories appended at the end)\n",
        "# - country: ordered by frequency (descending)\n",
        "# - Compact plots, consistent bar thickness across all plots (not ultra-thin)\n",
        "# - Fixed width for all plots; x-axis always 0–100 with 100% tick\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration\n",
        "# ----------------------------\n",
        "SINGLE_CHOICE = [\"exp_q\", \"exp_spl\", \"country\"]\n",
        "MULTI_RESPONSE = [\"phase\", \"role\", \"qc_areas\"]\n",
        "\n",
        "TOP_N_COUNTRY = 25       # set None for all countries\n",
        "TOP_N_OTHERS  = 25       # set None for all categories in other variables\n",
        "MULTI_METRIC  = \"pct_rows\"\n",
        "\n",
        "FIXED_WIDTH_IN = 7.0     # same width for all figures\n",
        "BAR_HEIGHT = 0.55        # consistent thickness across all plots (compact but readable)\n",
        "\n",
        "LABEL_FONTSIZE = 9\n",
        "TITLE_FONTSIZE = 10\n",
        "\n",
        "LEFT_MARGIN = 0.42\n",
        "RIGHT_MARGIN = 0.98\n",
        "TOP_MARGIN = 0.88\n",
        "BOTTOM_MARGIN = 0.10\n",
        "\n",
        "# Canonical order keys (English only; dataset values are assumed to be English)\n",
        "EXP_Q_CANON = [\"beginner\", \"intermediate\", \"advanced\", \"expert\"]\n",
        "PHASE_CANON = [\"analysis\", \"design\", \"coding\", \"testing\", \"execution\"]\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "def _tight_height(n_items: int):\n",
        "    \"\"\"\n",
        "    Compact height: close to one line per category + minimal padding.\n",
        "    \"\"\"\n",
        "    return max(1.6, 0.26 * n_items + 0.9)\n",
        "\n",
        "def _normalize(s):\n",
        "    return str(s).strip()\n",
        "\n",
        "def _rank_by_canonical_list(values, canon_list):\n",
        "    \"\"\"\n",
        "    Rank existing dataset values by matching substrings to canon_list items.\n",
        "    Example: \"Beginner (0-1y)\" matches \"beginner\".\n",
        "    Unmatched values are appended after matched ones, preserving their original order.\n",
        "    \"\"\"\n",
        "    canon = [c.lower() for c in canon_list]\n",
        "    ranks = []\n",
        "    base_unknown = len(canon) + 1000\n",
        "    u = 0\n",
        "\n",
        "    for v in values:\n",
        "        s = _normalize(v)\n",
        "        sl = s.lower()\n",
        "\n",
        "        # Find first canonical token contained in the value\n",
        "        idx = None\n",
        "        for i, token in enumerate(canon):\n",
        "            if token in sl:\n",
        "                idx = i\n",
        "                break\n",
        "\n",
        "        if idx is None:\n",
        "            ranks.append(base_unknown + u)\n",
        "            u += 1\n",
        "        else:\n",
        "            ranks.append(idx)\n",
        "\n",
        "    return ranks\n",
        "\n",
        "def _prep_single_choice(table: pd.DataFrame, col: str):\n",
        "    if \"category\" not in table.columns:\n",
        "        raise ValueError(f\"Table for '{col}' lacks 'category'.\")\n",
        "    if \"pct\" not in table.columns:\n",
        "        raise ValueError(f\"Single-choice table for '{col}' lacks 'pct'.\")\n",
        "\n",
        "    dfp = table.copy()\n",
        "    dfp[\"category\"] = dfp[\"category\"].astype(str).str.strip()\n",
        "    dfp[\"value\"] = pd.to_numeric(dfp[\"pct\"], errors=\"coerce\")\n",
        "    dfp = dfp.dropna(subset=[\"value\"])\n",
        "\n",
        "    if col == \"exp_q\":\n",
        "        # Logical order based on canonical levels found in the dataset values\n",
        "        dfp[\"rank\"] = _rank_by_canonical_list(dfp[\"category\"].tolist(), EXP_Q_CANON)\n",
        "        dfp = dfp.sort_values(\"rank\", ascending=True).drop(columns=[\"rank\"])\n",
        "\n",
        "    elif col == \"country\":\n",
        "        # Frequency order descending\n",
        "        dfp = dfp.sort_values(\"value\", ascending=False)\n",
        "\n",
        "    elif col == \"exp_spl\":\n",
        "        # Best effort: numeric if possible, else by frequency descending\n",
        "        # (If you want a specific ordinal order here, provide the labels and we can enforce it.)\n",
        "        cats = dfp[\"category\"].tolist()\n",
        "        numeric_ok = True\n",
        "        numeric_vals = []\n",
        "        for c in cats:\n",
        "            try:\n",
        "                numeric_vals.append(float(str(c)))\n",
        "            except Exception:\n",
        "                numeric_ok = False\n",
        "                break\n",
        "        if numeric_ok:\n",
        "            dfp[\"rank\"] = numeric_vals\n",
        "            dfp = dfp.sort_values(\"rank\", ascending=True).drop(columns=[\"rank\"])\n",
        "        else:\n",
        "            dfp = dfp.sort_values(\"value\", ascending=False)\n",
        "\n",
        "    else:\n",
        "        dfp = dfp.sort_values(\"value\", ascending=False)\n",
        "\n",
        "    # Apply TOP_N\n",
        "    top_n = TOP_N_COUNTRY if col == \"country\" else TOP_N_OTHERS\n",
        "    if top_n is not None and len(dfp) > top_n:\n",
        "        dfp = dfp.iloc[:top_n].copy()\n",
        "\n",
        "    # For horizontal bars: show first item at top -> reverse\n",
        "    dfp = dfp[[\"category\", \"value\"]].iloc[::-1].reset_index(drop=True)\n",
        "    return dfp\n",
        "\n",
        "def _prep_multi_response(table: pd.DataFrame, col: str):\n",
        "    if \"category\" not in table.columns:\n",
        "        raise ValueError(f\"Table for '{col}' lacks 'category'.\")\n",
        "    if MULTI_METRIC not in table.columns:\n",
        "        raise ValueError(f\"Multi-response table for '{col}' lacks '{MULTI_METRIC}'.\")\n",
        "\n",
        "    dfp = table.copy()\n",
        "    dfp[\"category\"] = dfp[\"category\"].astype(str).str.strip()\n",
        "    dfp[\"value\"] = pd.to_numeric(dfp[MULTI_METRIC], errors=\"coerce\")\n",
        "    dfp = dfp.dropna(subset=[\"value\"])\n",
        "\n",
        "    if col == \"phase\":\n",
        "        # Logical lifecycle order using dataset values (substring matching)\n",
        "        dfp[\"rank\"] = _rank_by_canonical_list(dfp[\"category\"].tolist(), PHASE_CANON)\n",
        "        dfp = dfp.sort_values(\"rank\", ascending=True).drop(columns=[\"rank\"])\n",
        "    else:\n",
        "        # Default: frequency order descending\n",
        "        dfp = dfp.sort_values(\"value\", ascending=False)\n",
        "\n",
        "    # Apply TOP_N\n",
        "    if TOP_N_OTHERS is not None and len(dfp) > TOP_N_OTHERS:\n",
        "        dfp = dfp.iloc[:TOP_N_OTHERS].copy()\n",
        "\n",
        "    # For horizontal bars: show first item at top -> reverse\n",
        "    dfp = dfp[[\"category\", \"value\"]].iloc[::-1].reset_index(drop=True)\n",
        "    return dfp\n",
        "\n",
        "def _plot_barh_compact(dfp: pd.DataFrame, title: str):\n",
        "    n = len(dfp)\n",
        "    plt.figure(figsize=(FIXED_WIDTH_IN, _tight_height(n)))\n",
        "    y = np.arange(n)\n",
        "\n",
        "    plt.barh(y, dfp[\"value\"].values, height=BAR_HEIGHT)\n",
        "    plt.yticks(y, dfp[\"category\"].values, fontsize=LABEL_FONTSIZE)\n",
        "\n",
        "    # Force x-axis 0..100 and show 100% explicitly\n",
        "    plt.xlim(0, 100)\n",
        "    plt.xticks([0, 25, 50, 75, 100])\n",
        "\n",
        "    plt.xlabel(\"Percent (%)\")\n",
        "    plt.ylabel(\"\")\n",
        "    plt.title(title, fontsize=TITLE_FONTSIZE)\n",
        "\n",
        "    plt.subplots_adjust(left=LEFT_MARGIN, right=RIGHT_MARGIN, top=TOP_MARGIN, bottom=BOTTOM_MARGIN)\n",
        "    plt.show()\n",
        "\n",
        "# ----------------------------\n",
        "# Run plots\n",
        "# ----------------------------\n",
        "if \"freq_tables\" not in globals() or not isinstance(freq_tables, dict):\n",
        "    raise NameError(\"freq_tables is not defined. Run the frequency-table cell first to build freq_tables.\")\n",
        "\n",
        "# Single-choice plots\n",
        "for col in SINGLE_CHOICE:\n",
        "    if col not in freq_tables:\n",
        "        print(f\"[WARN] '{col}' not found in freq_tables. Skipping.\")\n",
        "        continue\n",
        "    try:\n",
        "        dfp = _prep_single_choice(freq_tables[col], col)\n",
        "        _plot_barh_compact(dfp, f\"{col} — Horizontal bar plot (single-choice)\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not plot '{col}': {e}\")\n",
        "\n",
        "# Multi-response plots\n",
        "for col in MULTI_RESPONSE:\n",
        "    if col not in freq_tables:\n",
        "        print(f\"[WARN] '{col}' not found in freq_tables. Skipping.\")\n",
        "        continue\n",
        "    try:\n",
        "        dfp = _prep_multi_response(freq_tables[col], col)\n",
        "        _plot_barh_compact(dfp, f\"{col} — Horizontal bar plot (multi-response; metric={MULTI_METRIC})\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not plot '{col}': {e}\")\n"
      ],
      "metadata": {
        "id": "pqwYzZS2YsB-"
      },
      "id": "pqwYzZS2YsB-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AQUI TERMINA**"
      ],
      "metadata": {
        "id": "gT9irchqUo1G"
      },
      "id": "gT9irchqUo1G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**8.** Categorical Variables"
      ],
      "metadata": {
        "id": "eXjrN5htnWZS"
      },
      "id": "eXjrN5htnWZS"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Frequency tables for categorical variables (edit COLS accordingly)\n",
        "CATEGORICAL_COLS = [\"phase\", \"role\", \"exp_q\", \"exp_spl\", \"qc_areas\", \"country\"]  # e.g., [\"role\", \"country\", \"framework_primary\"]\n",
        "freq_tables = {}\n",
        "for col in CATEGORICAL_COLS:\n",
        "    vc = df[col].value_counts(dropna=False)\n",
        "    freq_tables[col] = vc\n",
        "    print(\"\\n=== {} ===\".format(col))\n",
        "    display(vc)"
      ],
      "metadata": {
        "id": "KF9jhQXTiscN"
      },
      "id": "KF9jhQXTiscN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8de655c1",
      "metadata": {
        "id": "8de655c1"
      },
      "source": [
        "## 6. Reliability analysis (Cronbach's alpha)\n",
        "\n",
        "**Purpose.** Cronbach’s alpha (α) estimates the **internal consistency** (reliability) of a set of items intended to measure the same construct. Higher α suggests that items share common variance and move together as a scale.\n",
        "\n",
        "**Computation.** $For (k) items (X_1,dots,X_k) scored numerically (e.g., Likert), and after listwise deletion of missing values, we compute:\n",
        "[\n",
        "\\alpha = \\frac{k}{k-1}\\Bigg(1 - \\frac{\\sum_{i=1}^{k} s_i^2}{s_T^2}\\Bigg)], where (s_i^2) is the sample variance of item (i), and (s_T^2) is the variance of the **row-wise sum** (T = \\sum_i X_i).$\n",
        "\n",
        "**Interpretation (rules of thumb, not hard cutoffs).**\n",
        "- **≥ 0.90**: Excellent  \n",
        "- **0.80–0.89**: Good  \n",
        "- **0.70–0.79**: Acceptable  \n",
        "- **0.60–0.69**: Questionable  \n",
        "- **0.50–0.59**: Poor  \n",
        "- **< 0.50**: Unacceptable\n",
        "\n",
        "**Reading the results table.**\n",
        "- **construct**: name of the latent variable you defined.  \n",
        "- **k_items**: number of items in the construct (≥ 2 required).  \n",
        "- **n_used**: number of responses used **after** listwise NA removal.  \n",
        "- **alpha**: Cronbach’s α (rounded to three decimals in the display).  \n",
        "- **interpretation**: qualitative label per the thresholds above.  \n",
        "- **notes**: quick guidance (e.g., possible item redundancy if α>0.95; consider item review if α<0.70).\n",
        "\n",
        "**Good practice and caveats.**\n",
        "- α assumes **tau-equivalence** and is sensitive to the **number of items** and **item variance** (longer scales often inflate α).  \n",
        "- A very high α (>0.95) can indicate **redundant items**.  \n",
        "- A low α may reflect **heterogeneous content**, **few items**, or **restricted variance** (e.g., ceiling effects).  \n",
        "- Always complement α with **item diagnostics** (e.g., item–total correlations, α if item deleted), **factor analysis**, and external **validity** checks.\n",
        "\n",
        "**Tip for Colab rendering.** Use a Markdown cell (not a code cell) to render LaTeX like \\(\\alpha\\) and \\(\\hat{p}\\). If generating Markdown from Python, wrap strings as raw literals (e.g., `r\"\"\"...\"\"\"`) to preserve backslashes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ebcf490",
      "metadata": {
        "id": "0ebcf490"
      },
      "outputs": [],
      "source": [
        "# --- Build CONSTRUCTS from previously computed `groups` and (optionally) compute Cronbach’s alpha ---\n",
        "# All code and comments in English.\n",
        "\n",
        "import pandas as pd\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "def _pretty_construct_name(group_key: str) -> str:\n",
        "    \"\"\"\n",
        "    Turn a group key like 'gen_process' into a readable construct name: 'Process'.\n",
        "    Keeps a short descriptor; you can adapt this to your naming conventions.\n",
        "    \"\"\"\n",
        "    key = str(group_key)\n",
        "    if key.startswith(\"gen_\"):\n",
        "        key = key[4:]\n",
        "    return key.replace(\"_\", \" \").strip().title()\n",
        "\n",
        "# 1) Build CONSTRUCTS directly from `groups` (features only, since c_* were already ignored)\n",
        "CONSTRUCTS = {}\n",
        "for gname, cols in groups.items():\n",
        "    # gname is the 'gen_*' column (group header). Use a pretty label for readability.\n",
        "    pretty = _pretty_construct_name(gname)\n",
        "    # Only keep non-empty lists\n",
        "    if cols:\n",
        "        CONSTRUCTS[pretty] = list(cols)\n",
        "\n",
        "# 2) Show a compact preview of the constructs (name -> #items)\n",
        "if not CONSTRUCTS:\n",
        "    display(Markdown(\"> **No constructs built from `groups`.** Check that `groups` is non-empty.\"))\n",
        "else:\n",
        "    display(Markdown(\"### Constructs built from `groups`\"))\n",
        "    preview_rows = [{\"construct\": k, \"k_items\": len(v)} for k, v in CONSTRUCTS.items()]\n",
        "    display(pd.DataFrame(preview_rows).sort_values(\"construct\").reset_index(drop=True))\n",
        "\n",
        "# 3) If `cronbach_alpha` is defined in the notebook, compute interpreted α per construct\n",
        "if \"cronbach_alpha\" in globals():\n",
        "    def _interpret_alpha(a: float) -> str:\n",
        "        if pd.isna(a): return \"NA\"\n",
        "        if a >= 0.90: return \"Excellent\"\n",
        "        if a >= 0.80: return \"Good\"\n",
        "        if a >= 0.70: return \"Acceptable\"\n",
        "        if a >= 0.60: return \"Questionable\"\n",
        "        if a >= 0.50: return \"Poor\"\n",
        "        return \"Unacceptable\"\n",
        "\n",
        "    records = []\n",
        "    for name, cols in CONSTRUCTS.items():\n",
        "        # Coerce to numeric and perform listwise deletion within the construct\n",
        "        sub = df[cols].apply(pd.to_numeric, errors=\"coerce\")\n",
        "        sub_clean = sub.dropna(axis=0, how=\"any\")\n",
        "        k_items = len(cols)\n",
        "        n_used = len(sub_clean)\n",
        "        a = cronbach_alpha(sub_clean) if (k_items > 1 and n_used > 1) else float(\"nan\")\n",
        "        note = \"\"\n",
        "        if pd.isna(a):\n",
        "            note = \"Insufficient items/variance or too many missing values.\"\n",
        "        elif a < 0.70:\n",
        "            note = \"Consider reviewing items; check item–total correlations.\"\n",
        "        elif a > 0.95:\n",
        "            note = \"Possible redundancy among items.\"\n",
        "\n",
        "        records.append({\n",
        "            \"construct\": name,\n",
        "            \"k_items\": k_items,\n",
        "            \"n_used\": n_used,\n",
        "            \"alpha\": a,\n",
        "            \"interpretation\": _interpret_alpha(a),\n",
        "            \"notes\": note\n",
        "        })\n",
        "\n",
        "    res = pd.DataFrame.from_records(records)\n",
        "    if not res.empty:\n",
        "        res_disp = res.copy()\n",
        "        res_disp[\"alpha\"] = res_disp[\"alpha\"].round(3)\n",
        "        display(Markdown(\"### Cronbach’s alpha by construct (interpreted)\"))\n",
        "        display(res_disp[[\"construct\", \"k_items\", \"n_used\", \"alpha\", \"interpretation\", \"notes\"]]\n",
        "                .sort_values([\"alpha\"], ascending=[False], na_position=\"last\")\n",
        "                .reset_index(drop=True))\n",
        "else:\n",
        "    display(Markdown(\"> `cronbach_alpha` is not defined in the current scope. \"\n",
        "                     \"Define it first to compute reliability for these constructs.\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###McDonald’s Omega (ω total)"
      ],
      "metadata": {
        "id": "4fafn9BrI8I6"
      },
      "id": "4fafn9BrI8I6"
    },
    {
      "cell_type": "code",
      "source": [
        "# === McDonald's Omega (ω) per construct with real per-branch n and missing-data handling ===\n",
        "# Works in Google Colab. Assumes `df` (responses DataFrame) and `CONSTRUCTS` (dict: name -> list of columns) exist.\n",
        "\n",
        "!pip -q install factor-analyzer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Optional: MINRES (requires complete cases per construct)\n",
        "from factor_analyzer import FactorAnalyzer\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration\n",
        "# ----------------------------\n",
        "# If True, always use PCA-based omega on Spearman (pairwise) -> leverages all available data (recommended with small n).\n",
        "# If False, try MINRES on rank-transformed data (complete cases); on failure, fallback to PCA.\n",
        "FORCE_PCA_ONLY = True\n",
        "\n",
        "# ----------------------------\n",
        "# Utilities\n",
        "# ----------------------------\n",
        "def _interpret_omega(w: float) -> str:\n",
        "    if pd.isna(w): return \"NA\"\n",
        "    if w >= 0.90: return \"Excellent\"\n",
        "    if w >= 0.80: return \"Good\"\n",
        "    if w >= 0.70: return \"Acceptable\"\n",
        "    if w >= 0.60: return \"Questionable\"\n",
        "    return \"Poor/Unacceptable\"\n",
        "\n",
        "def _pairwise_min_n(df_sub: pd.DataFrame) -> int:\n",
        "    \"\"\"Minimum number of non-missing pairs across all item pairs (support for pairwise Spearman).\"\"\"\n",
        "    cols = df_sub.columns.tolist()\n",
        "    if len(cols) == 0:\n",
        "        return 0\n",
        "    if len(cols) == 1:\n",
        "        return int(df_sub[cols[0]].notna().sum())\n",
        "    mins = []\n",
        "    for a, b in combinations(cols, 2):\n",
        "        mins.append(int(df_sub[[a, b]].dropna(how=\"any\").shape[0]))\n",
        "    return min(mins) if mins else 0\n",
        "\n",
        "def _omega_from_loadings_uniqueness(loadings: np.ndarray, uniqueness: np.ndarray) -> float:\n",
        "    loadings = np.asarray(loadings).flatten()\n",
        "    uniqueness = np.asarray(uniqueness).flatten()\n",
        "    if np.any(~np.isfinite(loadings)) or np.any(~np.isfinite(uniqueness)):\n",
        "        return np.nan\n",
        "    uniqueness = np.clip(uniqueness, 0.0, None)\n",
        "    num = (np.sum(loadings))**2\n",
        "    den = num + np.sum(uniqueness)\n",
        "    return float(num / den) if den > 0 else np.nan\n",
        "\n",
        "def _omega_pca_fallback(df_sub: pd.DataFrame) -> float:\n",
        "    \"\"\"\n",
        "    PCA-based omega on Spearman correlation (pairwise).\n",
        "    For 1-factor approximation: λ_i = sqrt(eig1) * v1_i, ψ_i = 1 - λ_i^2 (since R is a correlation matrix).\n",
        "    \"\"\"\n",
        "    # Pairwise Spearman correlation\n",
        "    R = df_sub.corr(method=\"spearman\").to_numpy()\n",
        "    # Symmetrize, clip, set diag to 1\n",
        "    R = (R + R.T) / 2.0\n",
        "    np.fill_diagonal(R, 1.0)\n",
        "    R = np.clip(R, -1.0, 1.0)\n",
        "\n",
        "    # Eigen-decomposition\n",
        "    vals, vecs = np.linalg.eigh(R)\n",
        "    idx = np.argsort(vals)[::-1]\n",
        "    eig1 = float(vals[idx[0]])\n",
        "    v1 = vecs[:, idx[0]]\n",
        "    # First component loadings\n",
        "    loadings = np.sqrt(max(eig1, 0.0)) * v1\n",
        "    communality = np.clip(loadings**2, 0.0, 1.0)\n",
        "    uniqueness = 1.0 - communality\n",
        "    return _omega_from_loadings_uniqueness(loadings, uniqueness)\n",
        "\n",
        "def _omega_minres_on_ranks(df_complete: pd.DataFrame) -> float:\n",
        "    \"\"\"\n",
        "    Fit 1-factor MINRES on rank-transformed data (Pearson on ranks ≈ Spearman).\n",
        "    Requires complete cases for the selected items.\n",
        "    \"\"\"\n",
        "    ranked = df_complete.rank(method=\"average\")\n",
        "    fa = FactorAnalyzer(n_factors=1, rotation=None, method=\"minres\")\n",
        "    fa.fit(ranked.values)\n",
        "    loadings = fa.loadings_.flatten()\n",
        "    uniqueness = fa.get_uniquenesses().flatten()\n",
        "    return _omega_from_loadings_uniqueness(loadings, uniqueness)\n",
        "\n",
        "def _compute_omega_for_construct(df: pd.DataFrame, cols: list, force_pca: bool = True):\n",
        "    \"\"\"\n",
        "    Returns (omega, n_used, note).\n",
        "    - If force_pca: uses PCA on Spearman (pairwise), n_used = min pairwise n among item pairs.\n",
        "    - Else: tries MINRES on complete cases; if fails, fallback to PCA; n_used reported accordingly.\n",
        "    \"\"\"\n",
        "    sub = df[cols].apply(pd.to_numeric, errors=\"coerce\")\n",
        "    k_items = len(cols)\n",
        "\n",
        "    if k_items < 2:\n",
        "        return np.nan, 0, \"Insufficient items (<2).\"\n",
        "\n",
        "    # Availability\n",
        "    n_total = len(sub)\n",
        "    n_complete = len(sub.dropna(how=\"any\"))\n",
        "    n_min_pair = _pairwise_min_n(sub)\n",
        "\n",
        "    if force_pca:\n",
        "        if n_min_pair < 4:\n",
        "            return np.nan, n_min_pair, \"Too few responses (pairwise) for ω.\"\n",
        "        try:\n",
        "            w = _omega_pca_fallback(sub)\n",
        "            note = \"PCA (pairwise Spearman).\"\n",
        "            if np.isfinite(w) and w > 0.95:\n",
        "                note += \" Possible redundancy among items.\"\n",
        "            elif np.isfinite(w) and w < 0.70:\n",
        "                note += \" Low common variance; consider reviewing items.\"\n",
        "            return w, n_min_pair, note\n",
        "        except Exception as e:\n",
        "            return np.nan, n_min_pair, f\"Estimation failed (PCA): {e}\"\n",
        "\n",
        "    # MINRES route (complete cases), then fallback\n",
        "    if n_complete >= 4:\n",
        "        try:\n",
        "            w = _omega_minres_on_ranks(sub.dropna(how=\"any\"))\n",
        "            note = \"MINRES on ranks (complete cases).\"\n",
        "            if not np.isfinite(w):\n",
        "                raise RuntimeError(\"MINRES returned non-finite ω.\")\n",
        "            if w > 0.95:\n",
        "                note += \" Possible redundancy among items.\"\n",
        "            elif w < 0.70:\n",
        "                note += \" Low common variance; consider reviewing items.\"\n",
        "            return w, n_complete, note\n",
        "        except Exception as e:\n",
        "            # Fallback to PCA (pairwise)\n",
        "            try:\n",
        "                w = _omega_pca_fallback(sub)\n",
        "                note = f\"PCA fallback (MINRES failed: {e}).\"\n",
        "                if np.isfinite(w) and w > 0.95:\n",
        "                    note += \" Possible redundancy among items.\"\n",
        "                elif np.isfinite(w) and w < 0.70:\n",
        "                    note += \" Low common variance; consider reviewing items.\"\n",
        "                return w, n_min_pair, note\n",
        "            except Exception as e2:\n",
        "                return np.nan, n_min_pair, f\"Estimation failed (both MINRES and PCA): {e}; fallback: {e2}\"\n",
        "    else:\n",
        "        # Not enough complete cases; use PCA (pairwise)\n",
        "        if n_min_pair < 4:\n",
        "            return np.nan, n_min_pair, \"Too few responses (pairwise) for ω.\"\n",
        "        try:\n",
        "            w = _omega_pca_fallback(sub)\n",
        "            note = \"PCA (pairwise Spearman) due to insufficient complete cases.\"\n",
        "            if np.isfinite(w) and w > 0.95:\n",
        "                note += \" Possible redundancy among items.\"\n",
        "            elif np.isfinite(w) and w < 0.70:\n",
        "                note += \" Low common variance; consider reviewing items.\"\n",
        "            return w, n_min_pair, note\n",
        "        except Exception as e:\n",
        "            return np.nan, n_min_pair, f\"Estimation failed (PCA): {e}\"\n",
        "\n",
        "# ----------------------------\n",
        "# Run: availability + omega by construct\n",
        "# ----------------------------\n",
        "if \"CONSTRUCTS\" not in globals() or not isinstance(CONSTRUCTS, dict) or len(CONSTRUCTS) == 0:\n",
        "    display(Markdown(\"> **No constructs available.** Please build `CONSTRUCTS` first.\"))\n",
        "else:\n",
        "    # Availability diagnostics\n",
        "    rows_diag = []\n",
        "    for name, cols in CONSTRUCTS.items():\n",
        "        sub = df[cols].apply(pd.to_numeric, errors=\"coerce\")\n",
        "        rows_diag.append({\n",
        "            \"construct\": name,\n",
        "            \"k_items\": len(cols),\n",
        "            \"n_total_rows\": len(sub),\n",
        "            \"n_complete_rows\": len(sub.dropna(how='any')),\n",
        "            \"n_min_pairwise\": _pairwise_min_n(sub),\n",
        "        })\n",
        "    diag_df = pd.DataFrame(rows_diag).sort_values(\"construct\").reset_index(drop=True)\n",
        "    display(Markdown(\"### Availability per construct\"))\n",
        "    display(diag_df)\n",
        "\n",
        "    # Omega computation\n",
        "    records = []\n",
        "    for name, cols in CONSTRUCTS.items():\n",
        "        w, n_used, note = _compute_omega_for_construct(df, cols, force_pca=FORCE_PCA_ONLY)\n",
        "        records.append({\n",
        "            \"construct\": name,\n",
        "            \"k_items\": len(cols),\n",
        "            \"n_used\": n_used,\n",
        "            \"omega_total\": w,\n",
        "            \"interpretation\": _interpret_omega(w),\n",
        "            \"notes\": note\n",
        "        })\n",
        "\n",
        "    res = pd.DataFrame.from_records(records)\n",
        "    if not res.empty:\n",
        "        res_disp = res.copy()\n",
        "        # Round omega for display\n",
        "        if \"omega_total\" in res_disp:\n",
        "            res_disp[\"omega_total\"] = res_disp[\"omega_total\"].round(3)\n",
        "        display(Markdown(\"### McDonald’s Omega (ω total) by construct\"))\n",
        "        display(res_disp.sort_values([\"omega_total\"], ascending=[False], na_position=\"last\").reset_index(drop=True))\n",
        "\n",
        "    # Reminder for small n\n",
        "    if not res.empty and (res[\"n_used\"] < 30).any():\n",
        "        display(Markdown(\n",
        "            \"> **Note:** Given the small effective n per construct, treat ω as exploratory and \"\n",
        "            \"report the per-branch n used (pairwise/complete) alongside results.\"\n",
        "        ))\n"
      ],
      "metadata": {
        "id": "IAg6LJOyI7h8"
      },
      "id": "IAg6LJOyI7h8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4b5259ee",
      "metadata": {
        "id": "4b5259ee"
      },
      "source": [
        "## 7. Cross-tabs & basic significance tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92055b4b",
      "metadata": {
        "id": "92055b4b"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Chi-square tests for association (categorical x categorical)\n",
        "PAIRS_CATEG = []  # e.g., [(\"role\", \"framework_primary\")]\n",
        "for a, b in PAIRS_CATEG:\n",
        "    ct = pd.crosstab(df[a], df[b])\n",
        "    chi2, p, dof, exp = stats.chi2_contingency(ct.fillna(0))\n",
        "    print(\"\\n{} vs {}\".format(a, b))\n",
        "    display(ct)\n",
        "    print(\"chi2={:.3f}, dof={}, p={:.4f}\".format(chi2, dof, p))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc32ab30",
      "metadata": {
        "id": "fc32ab30"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title T-test / ANOVA for numeric outcomes (edit as needed)\n",
        "# Example: Does perceived usefulness differ by role?\n",
        "# OUTCOME = \"fm_var_mgmt\"\n",
        "# GROUP = \"role\"\n",
        "# dropna_rows = df[[OUTCOME, GROUP]].dropna()\n",
        "# groups = [vals[OUTCOME].astype(float).values for _, vals in dropna_rows.groupby(GROUP)]\n",
        "# if len(groups) > 1:\n",
        "#     f, p = stats.f_oneway(*groups)\n",
        "#     print(\"ANOVA: F={:.3f}, p={:.4f}\".format(f, p))\n",
        "# else:\n",
        "#     print(\"Not enough groups for ANOVA\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24fbdbba",
      "metadata": {
        "id": "24fbdbba"
      },
      "source": [
        "## 8. Publication-ready tables & figures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00a191d9",
      "metadata": {
        "id": "00a191d9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Example: Likert stacked bar (5-point)\n",
        "# Provide a helper to transform Likert columns to percentages per label\n",
        "LIKERT_ORDER = [1, 2, 3, 4, 5]  # after mapping to numeric\n",
        "LIKERT_LABELS = [\"Strongly disagree\",\"Disagree\",\"Neutral\",\"Agree\",\"Strongly agree\"]\n",
        "\n",
        "def likert_table(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
        "    # expects numeric 1..5\n",
        "    table = []\n",
        "    for c in cols:\n",
        "        counts = df[c].value_counts().reindex(LIKERT_ORDER, fill_value=0)\n",
        "        perc = (counts / counts.sum() * 100).round(1)\n",
        "        row = perc.rename(c)\n",
        "        table.append(row)\n",
        "    out = pd.DataFrame(table, columns=LIKERT_ORDER)\n",
        "    out.columns = LIKERT_LABELS\n",
        "    return out\n",
        "\n",
        "# Example usage:\n",
        "LIKERT_COLS = []  # e.g., [\"fm_var_mgmt\", \"fm_algo_select\", \"fm_integration\"]\n",
        "if LIKERT_COLS:\n",
        "    tbl = likert_table(df, LIKERT_COLS)\n",
        "    display(tbl)\n",
        "    tbl.to_csv(OUTPUT_DIR / \"likert_table.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb9fbf49",
      "metadata": {
        "id": "bb9fbf49",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Plot: Likert stacked bars\n",
        "def plot_likert_stacked(table: pd.DataFrame, title: str = \"Perception of FM\"):\n",
        "    # table columns should be ordered as LIKERT_LABELS\n",
        "    ax = table.plot(kind=\"bar\", stacked=True, figsize=(10, 4))\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Item\")\n",
        "    plt.ylabel(\"Percentage\")\n",
        "    plt.legend(title=\"Response\", bbox_to_anchor=(1.0, 1.0))\n",
        "    plt.tight_layout()\n",
        "    out = OUTPUT_DIR / (\"likert_stacked_\" + title.replace(\" \", \"_\").lower() + \".png\")\n",
        "    plt.savefig(out, dpi=200)\n",
        "    plt.show()\n",
        "    print(\"Saved:\", out)\n",
        "\n",
        "# Example:\n",
        "# if LIKERT_COLS:\n",
        "#     tbl = likert_table(df, LIKERT_COLS)\n",
        "#     plot_likert_stacked(tbl, title=\"Usefulness of the Feature Model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7e3acd9",
      "metadata": {
        "id": "e7e3acd9"
      },
      "source": [
        "## 9. Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7cbab79",
      "metadata": {
        "id": "c7cbab79"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Save session info and a minimal requirements file\n",
        "import platform, subprocess\n",
        "\n",
        "session = {\n",
        "    \"python\": sys.version,\n",
        "    \"platform\": platform.platform(),\n",
        "    \"pandas\": pd.__version__,\n",
        "    \"numpy\": np.__version__,\n",
        "    \"matplotlib\": plt.matplotlib.__version__,\n",
        "    \"scipy\": stats.__version__ if hasattr(stats, '__version__') else None,\n",
        "    \"statsmodels\": sm.__version__,\n",
        "    \"gspread\": HAS_GSPREAD,\n",
        "}\n",
        "with open(OUTPUT_DIR / \"session_info.json\", \"w\") as f:\n",
        "    json.dump(session, f, indent=2)\n",
        "print(json.dumps(session, indent=2))\n",
        "\n",
        "# Minimal requirements (edit as needed)\n",
        "req = \"\"\"\n",
        "pandas\n",
        "numpy\n",
        "matplotlib\n",
        "scipy\n",
        "statsmodels\n",
        "gspread\n",
        "gspread-dataframe\n",
        "\"\"\"\n",
        "with open(OUTPUT_DIR / \"requirements.txt\", \"w\") as f:\n",
        "    f.write(req.strip() + \"\\n\")\n",
        "print(\"Wrote:\", OUTPUT_DIR / \"requirements.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "956c17f0",
      "metadata": {
        "id": "956c17f0"
      },
      "source": [
        "\n",
        "## Appendix: Paper-oriented narrative (to fill in)\n",
        "\n",
        "### A. Research Questions (RQs)\n",
        "- RQ1: *To what extent does the proposed Feature Model cover variability in functionalities, algorithms, frameworks and integration models for hybrid Q–C systems?*\n",
        "- RQ2: *What is the perceived usefulness and ease of use of the Feature Model among QSE experts?*\n",
        "- RQ3: *Which factors (e.g., role, experience, domain) explain differences in adoption intent?*\n",
        "\n",
        "### B. Method\n",
        "- Instrument: Google Forms survey (include sections and sample items).\n",
        "- Participants: N=..., sampling & inclusion criteria.\n",
        "- Measures: Likert items grouped in constructs; open-ended items.\n",
        "- Analysis: Reliability (Cronbach’s alpha), descriptive stats, cross-tabs & significance tests.\n",
        "\n",
        "### C. Results\n",
        "- Response flow and demographics.\n",
        "- Construct reliability and descriptive stats.\n",
        "- Key figures: Likert stacked bars; histograms; cross-tabs.\n",
        "\n",
        "### D. Discussion\n",
        "- Alignment with prior QSE/SPL research; implications for Feature Models in hybrid Q–C systems.\n",
        "- Threats to validity: construct, internal, external, conclusion validity.\n",
        "- Limitations and future work.\n",
        "\n",
        "### E. Reuse in the paper\n",
        "Export figures from `/content/outputs` and reference them in the manuscript. Tables (CSV) can be loaded into LaTeX or Word tables.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}